{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04983bda-2d74-4989-a41b-5cbac5bf16f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#base imports\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "poly_dir = os.path.abspath(os.path.join(os.getcwd(), 'data_poly'))\n",
    "sys.path.append(poly_dir)\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import itertools\n",
    "\n",
    "# package imports\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow.feather as feather\n",
    "import pickle\n",
    "from firebase_helper import *\n",
    "import math\n",
    "import asyncio\n",
    "from pandas.io.json import json_normalize\n",
    "import hashlib\n",
    "\n",
    "# local imports\n",
    "import data_poly.poly_getdata as poly_getdata\n",
    "import data_poly.poly_url as poly_url\n",
    "import data_poly.poly_helper as poly_helper\n",
    "\n",
    "\n",
    "# Suppress the UserWarning with a specific message\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"DataFrame columns are not unique, some columns will be omitted.\",\n",
    "    category=UserWarning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049fd872-c9aa-4492-9343-ad76abd748ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"/home/peterzerg/repos/PeterZergQuant/.ENV\")\n",
    "api_key = os.environ.get(\"POLYGON_APIKEY_MASTER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50006515-c613-4b87-bc50-734ad2a1cb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fb = firebase_helper()\n",
    "db = fb.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a919cac8-1ff5-497e-a20f-71c794e798bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#upload functions\n",
    "\n",
    "def cloud_upload_single(db, collection_name, doc_name, data_dict):\n",
    "    #upload data to db\n",
    "    doc_ref = db.collection(collection_name).document(doc_name)\n",
    "    doc_ref.set(data_dict)\n",
    "\n",
    "def chunks(data, size):\n",
    "    data_keys = list(data.keys())\n",
    "    for i in range(0, len(data_keys), size):\n",
    "        yield {k: data[k] for k in data_keys[i:i+size]}\n",
    "        \n",
    "def cloud_upload_seq(db, collection_name, data_dict, chunk_size=500):\n",
    "    sub_dicts = list(chunks(data_dict, chunk_size))\n",
    "    # Iterate through the sub_dicts\n",
    "    for sub_dict in tqdm(sub_dicts):\n",
    "        # Create a batch to batch the writes\n",
    "        batch = db.batch()\n",
    "        # Iterate through the key-value pairs in the sub_dict\n",
    "        for key, value in sub_dict.items():\n",
    "            # Set the document reference in the news_data collection using the key\n",
    "            doc_ref = db.collection(collection_name).document(str(key))\n",
    "            # Add the key-value pair to the batch\n",
    "            batch.set(doc_ref, value)\n",
    "        # Commit the batch\n",
    "        batch.commit()\n",
    "        \n",
    "async def write_document(doc_ref, data):\n",
    "    \"\"\"\n",
    "    a coroutine for writing a document to Firestore\n",
    "    \"\"\"\n",
    "    await doc_ref.set(data)\n",
    "\n",
    "async def write_batch(batch, collection_name, task_limit=12):\n",
    "    \"\"\"\n",
    "    write a batch of documents to Firestore asynchronously\n",
    "    \"\"\"\n",
    "    semaphore = asyncio.Semaphore(task_limit) # Create a semaphore to limit the number of concurrent tasks\n",
    "    coroutines = [] # Create a list to hold the coroutines\n",
    "    # Iterate through the batch and create a coroutine for each document\n",
    "    for key, value in batch.items():\n",
    "        # Set the document reference in the news_data collection using the key\n",
    "        doc_ref = db.collection(collection_name).document(str(key))\n",
    "        # Create a coroutine to write the document to Firestore\n",
    "        coroutine = write_document(doc_ref, value)\n",
    "        # Append the coroutine to the list\n",
    "        coroutines.append(coroutine)\n",
    "    # Run the coroutines concurrently with the semaphore\n",
    "    async with semaphore:\n",
    "        await asyncio.gather(*coroutines)\n",
    "\n",
    "async def cloud_upload(db, data_dict, collection_name, chunk_size=500, task_limit=12):\n",
    "    \"\"\"\n",
    "    upload data dict to collection\n",
    "    \"\"\"\n",
    "    sub_dicts = list(chunks(data_dict, chunk_size))\n",
    "    # Iterate through the sub_dicts\n",
    "    for sub_dict in tqdm(sub_dicts):\n",
    "        batch = {} # Create a batch to batch the writes\n",
    "        for key, value in sub_dict.items(): # Iterate through the key-value pairs in the sub_dict\n",
    "            batch[key] = value # Add the key-value pair to the batch\n",
    "        asyncio.run(write_batch(batch, collection_name, task_limit=task_limit)) # Run the write_batch coroutine asynchronously\n",
    "        \n",
    "def delete_collection(db, collection_name):\n",
    "    collection_ref = db.collection(collection_name)\n",
    "    # Get all documents in the collection\n",
    "    docs = collection_ref.stream()\n",
    "    # Delete each document in the collection\n",
    "    for doc in docs:\n",
    "        doc.reference.delete()\n",
    "    # Delete the collection\n",
    "    db.collection(collection_name).document().delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de023d6c-e68f-4ad0-87eb-414520854d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sic matching and ticker map cloud set\n",
    "\n",
    "def cloud_upload_ticker_map(db):\n",
    "    \"\"\"\n",
    "    upload and set ticker map (str->int) to cloud firestore \n",
    "    \"\"\"\n",
    "    ticker_map_path = \"/mnt/d/data/news/ticker_maping_dict.pkl\"\n",
    "    ticker_map_dict = pickle.load(open(ticker_map_path, \"rb\"))\n",
    "    #overwrite ticker mapping on db\n",
    "    doc_ref = db.collection('ticker_map').document('dict')\n",
    "    doc_ref.set(ticker_map_dict)\n",
    "\n",
    "def memoize(function):\n",
    "    \"\"\"\n",
    "    cache helper for speed optimization\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    def wrapper(input):\n",
    "        if input not in cache:\n",
    "            cache[input] = function(input)\n",
    "        return cache[input]\n",
    "    return wrapper\n",
    "\n",
    "@memoize\n",
    "def sic_match(input):\n",
    "    \"\"\"\n",
    "    takes a SIC code and return the 10 SIC industries string\n",
    "    \"\"\"\n",
    "    sic_codes = {\n",
    "        '01': 'agriculture',\n",
    "        '02': 'agriculture',\n",
    "        '07': 'agriculture',\n",
    "        '08': 'agriculture',\n",
    "        '09': 'agriculture',\n",
    "        '10': 'mining',\n",
    "        '11': 'mining',\n",
    "        '12': 'mining',\n",
    "        '13': 'mining',\n",
    "        '14': 'mining',\n",
    "        '15': 'construction',\n",
    "        '16': 'construction',\n",
    "        '17': 'construction',\n",
    "        **{f\"{i:02d}\": \"manufacturing\" for i in range(20, 40)},\n",
    "        **{f\"{i:02d}\": \"transportation\" for i in range(40, 50)},\n",
    "        '50': 'wholesale',\n",
    "        '51': 'wholesale',\n",
    "        **{f\"{i:02d}\": \"retail\" for i in range(52, 60)},\n",
    "        **{f\"{i:02d}\": \"finance\" for i in range(60, 68)},\n",
    "        **{f\"{i:02d}\": \"services\" for i in range(70, 90)},\n",
    "        **{f\"{i:02d}\": \"public_administration\" for i in range(91, 100)},\n",
    "    }\n",
    "    try:\n",
    "        return sic_codes[str(input)[0:2]]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid input. Please enter a two-character string matching a valid SIC code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deb7f1db-05cd-497e-9a31-ea0cd63a56d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate ticker info dict\n",
    "\n",
    "async def get_all_ticker_info(db):\n",
    "    ticker_map_dict = db.collection('tickers').document('ticker_hash').get().to_dict()\n",
    "    url_factory = poly_url.StockUrlFactory(api_key)\n",
    "    ticker_lc = ticker_map_dict.keys()\n",
    "    urls_dict = {ticker: url_factory.ReferenceData.ticker_info(url_factory, ticker) for ticker in ticker_lc}\n",
    "    df_dict = await poly_helper.get_data_from_urls(urls_dict)\n",
    "    return df_dict\n",
    "\n",
    "async def ticker_info_ready():\n",
    "    df_info_dict = await get_all_ticker_info(db)\n",
    "    upsert_dict = {ticker: df.to_dict('records')[0] for ticker, df in df_info_dict.items()}\n",
    "    return upsert_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d5ca5-9569-4886-8daf-c57c3e9265a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cloud_upload_ticker_info(db, upsert_dict):\n",
    "    cloud_upload_seq(db, \"ticker_info\", upsert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608f520-781a-4539-b8d8-f9b03f9ecd91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def info_dict_to_sic_map(df_info_dict):\n",
    "    df = pd.concat(df_info_dict)\n",
    "    sic_map_dict = {\n",
    "        str(ticker): sic_match(sic_code[:2]) if isinstance(sic_code, str) else None\n",
    "        for ticker, sic_code in zip(df.ticker, df.sic_code)\n",
    "    }\n",
    "    return sic_map_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21cda6f-3c62-4510-b8f9-76bb252d0211",
   "metadata": {
    "tags": []
   },
   "source": [
    "upload news data to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfe880-a0a5-4b6c-84f1-0ee9ac8e56cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload news data to db\n",
    "\n",
    "def convert_arrays_to_lists(value):\n",
    "    \"\"\"\n",
    "    Convert arrays to lists\n",
    "    \"\"\"\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return list(value)\n",
    "    return value\n",
    "\n",
    "def to_boolean_list(industries):\n",
    "    \"\"\"\n",
    "    convert the industries list to a boolean list\n",
    "    \"\"\"\n",
    "    return [col in industries for col in industry_cols]\n",
    "\n",
    "def process_news_data(db):\n",
    "    news_data = pd.read_feather(\"/mnt/d/data/news/local_us_equity_news\") # Load the news data\n",
    "    doc_ref = db.collection('ticker_sic_map').document('dict') # Get reference to the document\n",
    "    doc = doc_ref.get() # Retrieve the document data\n",
    "    # Check if the document exists\n",
    "    ticker_sic_map = doc.to_dict() if doc.exists else print(f\"No such document: {doc_ref.id}\")\n",
    "    #add 10 industry cols\n",
    "    news_data[\"industries\"] = news_data.tickers.apply(lambda tickers: [ticker_sic_map.get(ticker, None) for ticker in tickers])\n",
    "    # Define the industry column names and default values\n",
    "    industry_cols = list(set(['agriculture', 'mining', 'construction', 'manufacturing', 'transportation',\n",
    "                     'wholesale', 'retail', 'finance', 'services', 'public_administration']))\n",
    "    # Create a dataframe with the boolean values for each industry\n",
    "    boolean_df = pd.DataFrame(tqdm(news_data['industries'].apply(to_boolean_list).tolist()), columns=industry_cols)\n",
    "    # process the news_data df\n",
    "    news_data = news_data.reset_index(drop=True)\n",
    "    boolean_df = boolean_df.reset_index(drop=True)\n",
    "    news_data = pd.concat([news_data, boolean_df], axis=1)\n",
    "    news_data = news_data.applymap(convert_arrays_to_lists)\n",
    "    return news_data\n",
    "\n",
    "def new_data_to_dict(news_data):\n",
    "    # Convert the DataFrame to a dictionary format\n",
    "    news_data_dict = news_data.set_index('id').T.to_dict()\n",
    "    return news_data_dict\n",
    "\n",
    "def clean_news(news_data):\n",
    "    flattened_info=json_normalize(news_data[\"publisher\"])\n",
    "    news_data=pd.concat([news_data.drop('publisher', axis=1), flattened_info], axis=1)\n",
    "    cols_to_process = ['tickers', 'keywords', 'industries']\n",
    "    cols_to_process = ['tickers', 'keywords', 'industries']\n",
    "    for col in cols_to_process:\n",
    "        news_data[col] = news_data[col].apply(lambda lst:  tuple(lst) if lst is not None else None)\n",
    "    news_data.drop_duplicates(inplace=True)\n",
    "    return news_data\n",
    "\n",
    "def gen_10_industries_df(news_data):\n",
    "    industry_cols = list(set(['agriculture', 'mining', 'construction', 'manufacturing', 'transportation',\n",
    "                     'wholesale', 'retail', 'finance', 'services', 'public_administration']))\n",
    "    industry_data = {} # Create a dictionary to hold the smaller dataframes\n",
    "    for col in industry_cols: # Iterate through the industry columns\n",
    "        # Get the most recent 100 rows where the industry boolean column is true\n",
    "        industry_rows = news_data[news_data[col] == True].sort_values(by='published_utc', ascending=False).head(100)\n",
    "        # Add the dataframe to the dictionary using the column name as the key\n",
    "        industry_data[col] = industry_rows.set_index('id').T.to_dict()\n",
    "        # Print the number of rows added to the dataframe\n",
    "        print(f\"{col}: {len(industry_rows)} rows added\")\n",
    "    return industry_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f029986-510e-4832-b50f-b88eb91c5142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_data = process_news_data(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6729e2-ce65-4673-aa64-1da614b4afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map ticker to industry (OK)\n",
    "#get tickers col as list from master news dataframe (rows) (OK)\n",
    "#map tickers in list 10 industries boolean col (OK)\n",
    "#create 10 industry dataframes, 100 rows each, order by time (OK)\n",
    "#write a python program that run every x mins to get new news and append to the master dataframe, and also append to the 10 industries dataframes.\n",
    "#user requests for news -> look up pref in firebase by user hashid -> merge industries dataframes -> push to front end 100 rows but limit displace 10 rows at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b286946-8dde-4f96-9700-fa6b22f1e2bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba62c6c-41c8-4efc-b1d3-289f3c795273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_data = news_data.tickers.apply(lambda tickers: tuple(itertools.combinations(tickers, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1a232-a345-428d-9515-c37ceb79051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59211f0-1cbc-4c11-97fb-0af9c7c1d591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import concurrent.futures\n",
    "\n",
    "def df_modify(tuple_pair, df, ref_map):\n",
    "    df.loc[ref_map[tuple_pair[0]], ref_map[tuple_pair[1]]] += 1\n",
    "\n",
    "def process_row(row, df, ref_map):\n",
    "    for tuple_pair in row:\n",
    "        df_modify(tuple_pair, df, ref_map)\n",
    "\n",
    "def process_data(target_data, df, ref_map, max_workers=12):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_row, row, df, ref_map) for row in target_data]\n",
    "        concurrent.futures.wait(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ec752-f23d-4365-bf51-2a2b09df44b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_map = db.collection('ticker_map').document('dict').get().to_dict()\n",
    "n = len(ref_map)\n",
    "index_list = list(range(n))\n",
    "df = pd.DataFrame(0, index=index_list, columns=index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dac9877-64be-4010-af82-567cf4c7e403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "process_data(target_data, df, ref_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46f20246-6f32-4c80-93c7-d22da9f36bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_factory = poly_url.StockUrlFactory(api_key)\n",
    "url = url_factory.ReferenceData.tickers(url_factory)\n",
    "tickers =  poly_helper.get_data_from_single_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f533b56-a2e1-4256-bc34-949b61fd12c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ticker_map_dict_gen():\n",
    "    # Define a function to create the hashid\n",
    "    def create_hashid(row):\n",
    "        cik = row['cik'] if row['cik'] else ''\n",
    "        composite_figi = row['composite_figi'] if row['composite_figi'] else ''\n",
    "        hash_str = f\"{row['ticker']}{cik}{composite_figi}\"\n",
    "        return hashlib.md5(hash_str.encode()).hexdigest()\n",
    "\n",
    "    #get tickers on us equity market:\n",
    "    url_factory = poly_url.StockUrlFactory(api_key)\n",
    "    url = url_factory.ReferenceData.tickers(url_factory)\n",
    "    tickers =  poly_helper.get_data_from_single_url(url)\n",
    "    # Apply the function to each row to create a hashid column\n",
    "    tickers['hashid'] = tickers.apply(create_hashid, axis=1)\n",
    "    ticker_map_dict = tickers[['ticker', 'hashid']].set_index('ticker').to_dict()['hashid']\n",
    "    return ticker_map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6e2bcc8-050c-4a9f-afad-1176a7326248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ticker_map_dict = tickers[['ticker', 'hashid']].set_index('ticker').to_dict()['hashid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14ea2f64-05fc-4a78-a9f3-2fcbdb540cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cloud_upload_single(db, \"tickers\", \"ticker_hash\", ticker_map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0c4448d-3a1c-4e9e-be17-10aec186b311",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeleted collection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mdelete_collections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mticker_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 12\u001b[0m, in \u001b[0;36mdelete_collections\u001b[0;34m(db, collection_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m     deleted \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deleted \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdelete_collections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m collection_ref\u001b[38;5;241m.\u001b[39mdelete()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeleted collection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 12\u001b[0m, in \u001b[0;36mdelete_collections\u001b[0;34m(db, collection_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m     deleted \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deleted \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdelete_collections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m collection_ref\u001b[38;5;241m.\u001b[39mdelete()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeleted collection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "    \u001b[0;31m[... skipping similar frames: delete_collections at line 12 (14 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[45], line 12\u001b[0m, in \u001b[0;36mdelete_collections\u001b[0;34m(db, collection_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m     deleted \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deleted \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdelete_collections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m collection_ref\u001b[38;5;241m.\u001b[39mdelete()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeleted collection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcollection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 8\u001b[0m, in \u001b[0;36mdelete_collections\u001b[0;34m(db, collection_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m deleted \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     deleted \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deleted \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids/lib/python3.8/site-packages/google/cloud/firestore_v1/document.py:354\u001b[0m, in \u001b[0;36mDocumentReference.delete\u001b[0;34m(self, option, retry, timeout)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Delete the current document in the Firestore database.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    still return the time that the request was received by the server.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m request, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prep_delete(option, retry, timeout)\n\u001b[0;32m--> 354\u001b[0m commit_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_firestore_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rpc_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m commit_response\u001b[38;5;241m.\u001b[39mcommit_time\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids/lib/python3.8/site-packages/google/cloud/firestore_v1/services/firestore/client.py:1125\u001b[0m, in \u001b[0;36mFirestoreClient.commit\u001b[0;34m(self, request, database, writes, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(metadata) \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m   1121\u001b[0m     gapic_v1\u001b[38;5;241m.\u001b[39mrouting_header\u001b[38;5;241m.\u001b[39mto_grpc_metadata(((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabase\u001b[39m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mdatabase),)),\n\u001b[1;32m   1122\u001b[0m )\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m-> 1125\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids/lib/python3.8/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids/lib/python3.8/site-packages/google/api_core/retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    348\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids/lib/python3.8/site-packages/google/api_core/retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids/lib/python3.8/site-packages/google/api_core/timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids/lib/python3.8/site-packages/google/api_core/grpc_helpers.py:72\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids/lib/python3.8/site-packages/grpc/_channel.py:944\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    938\u001b[0m              request,\n\u001b[1;32m    939\u001b[0m              timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m              wait_for_ready\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    943\u001b[0m              compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 944\u001b[0m     state, call, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids/lib/python3.8/site-packages/grpc/_channel.py:933\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    926\u001b[0m     call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[1;32m    927\u001b[0m         cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method, \u001b[38;5;28;01mNone\u001b[39;00m, _determine_deadline(deadline), metadata,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    932\u001b[0m         ),), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context)\n\u001b[0;32m--> 933\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m     _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:338\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:169\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:163\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:62\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:58\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._interpret_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/tag.pyx.pxi:71\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._BatchOperationTag.event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/operation.pyx.pxi:138\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.ReceiveInitialMetadataOperation.un_c\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:69\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._metadata\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:70\u001b[0m, in \u001b[0;36mgenexpr\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/metadata.pyx.pxi:64\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._metadatum\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m__new__\u001b[0;34m(_cls, key, value)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def delete_collections(db, collection_name):\n",
    "    collection_ref = db.collection(collection_name)\n",
    "    batch_size = 500  # batch size for deleting documents\n",
    "    docs = collection_ref.limit(batch_size).stream()\n",
    "\n",
    "    deleted = 0\n",
    "    for doc in docs:\n",
    "        doc.reference.delete()\n",
    "        deleted += 1\n",
    "\n",
    "    if deleted >= batch_size:\n",
    "        return delete_collections(db, collection_name)\n",
    "\n",
    "    collection_ref.delete()\n",
    "    print(f'Deleted collection {collection_name}')\n",
    "    return True\n",
    "\n",
    "delete_collections(db, \"ticker_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0429f-eaf4-44af-aac4-5fc149bc4a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cloud_upload_single(db, \"#update_time\", \"backend_codebase\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f10ed17d-ef41-4304-8390-4cb9b96f5fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAQC\n",
      "AAQC.U\n",
      "AAQC.WS\n",
      "ABGI\n",
      "ABMD\n",
      "ACDI\n",
      "ACDI.U\n",
      "ACDI.WS\n",
      "ACEV\n",
      "ACEVU\n",
      "ACEVW\n",
      "ACII\n",
      "ACII.U\n",
      "ACQR\n",
      "ACQRU\n",
      "ACQRW\n",
      "ACWF\n",
      "ADRA.U\n",
      "ADRA.WS\n",
      "AEAC\n",
      "AEACU\n",
      "AEACW\n",
      "AEHA\n",
      "AEHAU\n",
      "AEHAW\n",
      "AEPPZ\n",
      "AERC\n",
      "AERI\n",
      "AESE\n",
      "AFAC\n",
      "AFACU\n",
      "AFACW\n",
      "AFAQ\n",
      "AFAQU\n",
      "AFAQW\n",
      "AGAC.WS\n",
      "AGBAR\n",
      "AGBAU\n",
      "AGCB\n",
      "AGGR\n",
      "AGGRU\n",
      "AGGRW\n",
      "AGTC\n",
      "AIKI\n",
      "AKIC\n",
      "AKICU\n",
      "AKICW\n",
      "AKUS\n",
      "ALBO\n",
      "AMCI\n",
      "AMCIU\n",
      "AMCIW\n",
      "AMOV\n",
      "AMPI\n",
      "AMPI.U\n",
      "AMPI.WS\n",
      "ANAC\n",
      "ANAC.U\n",
      "ANAC.WS\n",
      "APN\n",
      "APN.U\n",
      "APN.WS\n",
      "APXH\n",
      "ARCK\n",
      "ARCKU\n",
      "ARCKW\n",
      "ARGU\n",
      "ARGUU\n",
      "ARGUW\n",
      "ASAX\n",
      "ASAXU\n",
      "ASAXW\n",
      "ASZ\n",
      "ASZ.U\n",
      "ATA\n",
      "ATA.U\n",
      "ATA.WS\n",
      "ATAQ.WS\n",
      "ATAX\n",
      "AUBAP\n",
      "AUS\n",
      "AUS.U\n",
      "AVCO\n",
      "AVCT\n",
      "AVCTW\n",
      "AVEO\n",
      "AVYA\n",
      "AXH\n",
      "AXH.U\n",
      "AXH.WS\n",
      "AYLA\n",
      "BACA.U\n",
      "BACA.WS\n",
      "BAMH\n",
      "BAMI\n",
      "BAMR\n",
      "BCOR\n",
      "BGSX\n",
      "BGSX.U\n",
      "BGSX.WS\n",
      "BIOT\n",
      "BIOTU\n",
      "BIOTW\n",
      "BLNKW\n",
      "BLTS\n",
      "BLTSU\n",
      "BLTSW\n",
      "BNFT\n",
      "BOB\n",
      "BPYPM\n",
      "BRMK.WS\n",
      "BSBE\n",
      "BSCM\n",
      "BSFFF\n",
      "BSJM\n",
      "BSKY\n",
      "BSKYU\n",
      "BSKYW\n",
      "BSMM\n",
      "BTCR\n",
      "BTN\n",
      "BTRS\n",
      "CAJ\n",
      "CCNC\n",
      "CEA\n",
      "CECE\n",
      "CENQ\n",
      "CENQU\n",
      "CENQW\n",
      "CEY\n",
      "CGABL\n",
      "CHAA.WS\n",
      "CHAD\n",
      "CHG\n",
      "CHSCL\n",
      "CHSCM\n",
      "CHSCN\n",
      "CHSCO\n",
      "CHSCP\n",
      "CINC\n",
      "CIXX\n",
      "CLAA\n",
      "CLAA.U\n",
      "CLAA.WS\n",
      "CLAS\n",
      "CLAS.U\n",
      "CLAS.WS\n",
      "CLIM\n",
      "CLIM.U\n",
      "CLR\n",
      "CLRM\n",
      "CLRMU\n",
      "CLRMW\n",
      "CLVS\n",
      "CMCTP\n",
      "CNCE\n",
      "CND\n",
      "CND.U\n",
      "CND.WS\n",
      "COLI\n",
      "COLIU\n",
      "COLIW\n",
      "CORZ\n",
      "CORZW\n",
      "COUP\n",
      "COVA\n",
      "COVAU\n",
      "COVAW\n",
      "COWN\n",
      "COWNL\n",
      "CPAQ\n",
      "CPAQU\n",
      "CPAQW\n",
      "CPAR\n",
      "CPARU\n",
      "CPARW\n",
      "CPTK.WS\n",
      "CRHC\n",
      "CRHC.U\n",
      "CRHC.WS\n",
      "CRU\n",
      "CRU.U\n",
      "CRWGD\n",
      "CTAQ\n",
      "CTAQU\n",
      "CTAQW\n",
      "CYBE\n",
      "CYRN\n",
      "CZOO.WS\n",
      "DAOO\n",
      "DAOOU\n",
      "DAOOW\n",
      "DBS\n",
      "DBV\n",
      "DCRD\n",
      "DCRDU\n",
      "DCRDW\n",
      "DDF\n",
      "DEX\n",
      "DGL\n",
      "DHBC\n",
      "DHBCU\n",
      "DHBCW\n",
      "DILA\n",
      "DILAU\n",
      "DILAW\n",
      "DLCA\n",
      "DLCAU\n",
      "DLCAW\n",
      "DNAY\n",
      "DNZ\n",
      "DNZ.U\n",
      "DNZ.WS\n",
      "DOZR\n",
      "DRAY\n",
      "DRAYU\n",
      "DRAYW\n",
      "DS\n",
      "DSAQ.WS\n",
      "DSPC\n",
      "DSpB\n",
      "DSpC\n",
      "DSpD\n",
      "DTRT\n",
      "DTRTU\n",
      "DTRTW\n",
      "EBAC\n",
      "EBACU\n",
      "EBACW\n",
      "ECOM\n",
      "EFHTU\n",
      "EKAR\n",
      "ELAT\n",
      "ELVT\n",
      "EMCF\n",
      "EPHY\n",
      "EPHYU\n",
      "EPHYW\n",
      "EPWR\n",
      "EPWR.U\n",
      "EPWR.WS\n",
      "EQHA\n",
      "EQHA.U\n",
      "EQHA.WS\n",
      "EQOS\n",
      "EQTNP\n",
      "ESM\n",
      "ESM.U\n",
      "ESM.WS\n",
      "ESSC\n",
      "ESSCR\n",
      "ESSCU\n",
      "ESSCW\n",
      "ETPA\n",
      "EVGBC\n",
      "EVK\n",
      "EVLMC\n",
      "EVSTC\n",
      "EXN\n",
      "FACA\n",
      "FACA.U\n",
      "FACA.WS\n",
      "FBC\n",
      "FBHS\n",
      "FCAX\n",
      "FCAX.U\n",
      "FCAX.WS\n",
      "FCNCO\n",
      "FCNCP\n",
      "FCRD\n",
      "FEMA\n",
      "FEO\n",
      "FEVR\n",
      "FFHL\n",
      "FHS\n",
      "FINM\n",
      "FINMU\n",
      "FINMW\n",
      "FLAC\n",
      "FLACU\n",
      "FLACW\n",
      "FLYA\n",
      "FLYA.U\n",
      "FLYA.WS\n",
      "FNHC\n",
      "FNI\n",
      "FNTC\n",
      "FOMO\n",
      "FOXW\n",
      "FOXWU\n",
      "FOXWW\n",
      "FPAC\n",
      "FPAC.U\n",
      "FPAC.WS\n",
      "FRON\n",
      "FRONU\n",
      "FRONW\n",
      "FRSG\n",
      "FRSGU\n",
      "FRSGW\n",
      "FRW\n",
      "FRWAU\n",
      "FRWAW\n",
      "FSNB.WS\n",
      "FSRD\n",
      "FSRDW\n",
      "FSSI\n",
      "FSSIU\n",
      "FSSIW\n",
      "FSTX\n",
      "FTAA\n",
      "FTAAU\n",
      "FTAAW\n",
      "FTCV\n",
      "FTCVU\n",
      "FTCVW\n",
      "FTEV\n",
      "FTEV.U\n",
      "FTEV.WS\n",
      "FTPA\n",
      "FTPAU\n",
      "FTPAW\n",
      "FTVI\n",
      "FTVIU\n",
      "FTVIW\n",
      "FUE\n",
      "FVIV\n",
      "FVIV.U\n",
      "FVT\n",
      "FVT.U\n",
      "FWP\n",
      "GACQ\n",
      "GACQU\n",
      "GACQW\n",
      "GAPA\n",
      "GAPA.U\n",
      "GBUG\n",
      "GCIG\n",
      "GEGGL\n",
      "GENI.WS\n",
      "GET\n",
      "GFLU\n",
      "GGMC\n",
      "GGMCU\n",
      "GGMCW\n",
      "GHAC\n",
      "GHACU\n",
      "GHACW\n",
      "GIAC\n",
      "GIACU\n",
      "GIACW\n",
      "GIIX\n",
      "GIIXU\n",
      "GIIXW\n",
      "GIW\n",
      "GIWWU\n",
      "GIWWW\n",
      "GLBL\n",
      "GLBLU\n",
      "GLBLW\n",
      "GLHA\n",
      "GLHAU\n",
      "GLHAW\n",
      "GLS.WS\n",
      "GMTX\n",
      "GNAC\n",
      "GNACU\n",
      "GNACW\n",
      "GRU\n",
      "GSEV\n",
      "GSEVU\n",
      "GSEVW\n",
      "GSQD\n",
      "GSQD.U\n",
      "GTHDF\n",
      "GTPA\n",
      "GTPAU\n",
      "GTPAW\n",
      "GTPB\n",
      "GTPBU\n",
      "GTPBW\n",
      "GWII\n",
      "GWIIW\n",
      "HAAC\n",
      "HAACU\n",
      "HAACW\n",
      "HAPP\n",
      "HCAR\n",
      "HCARU\n",
      "HCARW\n",
      "HCIC\n",
      "HCICU\n",
      "HCICW\n",
      "HCII\n",
      "HCIIU\n",
      "HCIIW\n",
      "HDIV\n",
      "HERA\n",
      "HERAU\n",
      "HERAW\n",
      "HIII\n",
      "HIIIU\n",
      "HIIIW\n",
      "HIL\n",
      "HLAH\n",
      "HLAHU\n",
      "HLAHW\n",
      "HMCO\n",
      "HMCOU\n",
      "HMCOW\n",
      "HMLPpA\n",
      "HORI\n",
      "HORIU\n",
      "HORIW\n",
      "HPX\n",
      "HPX.U\n",
      "HPX.WS\n",
      "HSAQ\n",
      "HTIBP\n",
      "HTPA\n",
      "HTPA.U\n",
      "HTPA.WS\n",
      "HUGS\n",
      "HUGS.U\n",
      "HUGS.WS\n",
      "HYLV\n",
      "HYRE\n",
      "HZN\n",
      "IACC\n",
      "IBDN\n",
      "IBER\n",
      "IBER.U\n",
      "IBER.WS\n",
      "IBHB\n",
      "IBMK\n",
      "IBTB\n",
      "IDIV\n",
      "IDRA\n",
      "IGAC\n",
      "IGACU\n",
      "IGACW\n",
      "IHCNF\n",
      "IHREF\n",
      "IIII\n",
      "IIIIU\n",
      "IIIIW\n",
      "IIVIP\n",
      "IMGO\n",
      "IMLP\n",
      "IMRA\n",
      "INAQ.WS\n",
      "IPAX\n",
      "IPAXU\n",
      "IPAXW\n",
      "IPVA\n",
      "IPVA.U\n",
      "IPVA.WS\n",
      "IRHG\n",
      "IRL\n",
      "IS\n",
      "ISAA\n",
      "ISAPF\n",
      "ISDMF\n",
      "ISMJF\n",
      "ISR\n",
      "ISUPF\n",
      "ISVTF\n",
      "ISVYF\n",
      "ITQ\n",
      "ITQRU\n",
      "ITQRW\n",
      "IUSA\n",
      "IVC\n",
      "IVH\n",
      "JAGG\n",
      "JBPCF\n",
      "JCIC\n",
      "JCICU\n",
      "JCICW\n",
      "JEMD\n",
      "JIGB\n",
      "JOFF\n",
      "JOFFU\n",
      "JOFFW\n",
      "JPHY\n",
      "JYAC\n",
      "KAHC\n",
      "KAHC.U\n",
      "KAHC.WS\n",
      "KAII\n",
      "KAIIU\n",
      "KAIIW\n",
      "KAIR\n",
      "KAIRU\n",
      "KAIRW\n",
      "KIII\n",
      "KIIIU\n",
      "KIIIW\n",
      "KINZ\n",
      "KINZU\n",
      "KINZW\n",
      "KLAQ\n",
      "KLAQU\n",
      "KLAQW\n",
      "KLCD\n",
      "KMPH\n",
      "KNBE\n",
      "KSCD\n",
      "KSI\n",
      "KSICU\n",
      "KSICW\n",
      "LAAA\n",
      "LAAAU\n",
      "LAAAW\n",
      "LAX\n",
      "LAXXR\n",
      "LAXXU\n",
      "LAXXW\n",
      "LEGA\n",
      "LEGAU\n",
      "LEGAW\n",
      "LFG\n",
      "LGAC\n",
      "LGACU\n",
      "LGACW\n",
      "LGTO\n",
      "LGTOU\n",
      "LGTOW\n",
      "LGV\n",
      "LGV.U\n",
      "LGV.WS\n",
      "LHAA\n",
      "LHC.WS\n",
      "LHCG\n",
      "LHDX\n",
      "LION\n",
      "LIONU\n",
      "LIONW\n",
      "LJAQ\n",
      "LJAQU\n",
      "LJAQW\n",
      "LMACA\n",
      "LMACU\n",
      "LMACW\n",
      "LNDC\n",
      "LOGC\n",
      "LOKM\n",
      "LOKM.U\n",
      "LOKM.WS\n",
      "LOTZ\n",
      "LOTZW\n",
      "LPI\n",
      "LSLPF\n",
      "LSPR\n",
      "LSPRU\n",
      "LSPRW\n",
      "LYL\n",
      "MACC\n",
      "MACC.U\n",
      "MBNKP\n",
      "MCAE\n",
      "MCAER\n",
      "MCAEU\n",
      "MCRO\n",
      "MDEVF\n",
      "MDH\n",
      "MDH.U\n",
      "MDH.WS\n",
      "MEAC\n",
      "MEACU\n",
      "MEACW\n",
      "MFGP\n",
      "MGU\n",
      "MHpA\n",
      "MHpC\n",
      "MHpD\n",
      "MICT\n",
      "MIMO.WS.C\n",
      "MIT\n",
      "MIT.U\n",
      "MIT.WS\n",
      "MITO\n",
      "MJIN\n",
      "MJXL\n",
      "MLAI\n",
      "MLAIU\n",
      "MLAIW\n",
      "MMX\n",
      "MNRL\n",
      "MON\n",
      "MONCU\n",
      "MONCW\n",
      "MPAC\n",
      "MPACR\n",
      "MPACU\n",
      "MPACW\n",
      "MSAC\n",
      "MSACW\n",
      "MSPR\n",
      "MSPRW\n",
      "MSPRZ\n",
      "MTBC\n",
      "MTBCO\n",
      "MTBCP\n",
      "MTMT\n",
      "MYOV\n",
      "NAAC\n",
      "NAACU\n",
      "NAACW\n",
      "NDAC\n",
      "NDACU\n",
      "NDACW\n",
      "NEEpP\n",
      "NFTZ\n",
      "NILE\n",
      "NILEpD\n",
      "NLOK\n",
      "NOAC\n",
      "NOACU\n",
      "NOACW\n",
      "NSTC.WS\n",
      "NVSA\n",
      "NVSAU\n",
      "NVSAW\n",
      "OEPW\n",
      "OEPWU\n",
      "OEPWW\n",
      "OG\n",
      "OHPA\n",
      "OHPAU\n",
      "OHPAW\n",
      "OIIM\n",
      "OMEG\n",
      "ONBPO\n",
      "ONBPP\n",
      "ONEM\n",
      "OPALW\n",
      "OPNT\n",
      "OSTR\n",
      "OSTRU\n",
      "OSTRW\n",
      "OYST\n",
      "PACX\n",
      "PACXU\n",
      "PACXW\n",
      "PAFO\n",
      "PAFOR\n",
      "PAFOU\n",
      "PAYA\n",
      "PBCRF\n",
      "PBFX\n",
      "PBUG\n",
      "PCPC\n",
      "PCPC.U\n",
      "PCPC.WS\n",
      "PCSB\n",
      "PCX\n",
      "PCXCU\n",
      "PCXCW\n",
      "PDOT\n",
      "PDOT.U\n",
      "PDOT.WS\n",
      "PEI\n",
      "PEIpB\n",
      "PEIpC\n",
      "PEIpD\n",
      "PFDR\n",
      "PFDRU\n",
      "PFDRW\n",
      "PFHD\n",
      "PFXNL\n",
      "PHAS\n",
      "PHIC\n",
      "PHICU\n",
      "PHICW\n",
      "PHYMF\n",
      "PICC.U\n",
      "PME\n",
      "POND\n",
      "POND.U\n",
      "POND.WS\n",
      "PONO\n",
      "PONOU\n",
      "PONOW\n",
      "POSH\n",
      "POW\n",
      "POWRU\n",
      "POWRW\n",
      "PQIN\n",
      "PRBM.U\n",
      "PRTY\n",
      "PSAG\n",
      "PSAGU\n",
      "PSAGW\n",
      "PSBpX\n",
      "PSBpY\n",
      "PSBpZ\n",
      "PTIC\n",
      "PTICU\n",
      "PTICW\n",
      "PTNR\n",
      "PTOC\n",
      "PTOCU\n",
      "PTOCW\n",
      "PV\n",
      "PV.U\n",
      "PV.WS\n",
      "PYS\n",
      "QED\n",
      "QLS\n",
      "QMN\n",
      "QNGY\n",
      "QNGY.WS\n",
      "QTNT\n",
      "QUMU\n",
      "RACB\n",
      "RADA\n",
      "RCII\n",
      "RCOR\n",
      "RCOR.WS\n",
      "RELFF\n",
      "REVBU\n",
      "REVH\n",
      "REVHU\n",
      "REVHW\n",
      "RFP\n",
      "RIGZ\n",
      "RIVr\n",
      "RJAC.WS\n",
      "RKLY\n",
      "RKLY.WS\n",
      "RKTA.WS\n",
      "RNER\n",
      "RNERU\n",
      "RNERW\n",
      "RNWK\n",
      "RODI\n",
      "ROXIF\n",
      "RSX\n",
      "RSXJ\n",
      "RXRA\n",
      "RXRAU\n",
      "RXRAW\n",
      "RZA\n",
      "SAITW\n",
      "SBII\n",
      "SBII.U\n",
      "SBMFF\n",
      "SBTX\n",
      "SBUG\n",
      "SCLE\n",
      "SCLEU\n",
      "SCLEW\n",
      "SCMA\n",
      "SCMAU\n",
      "SCMAW\n",
      "SCOA\n",
      "SCOAU\n",
      "SCOAW\n",
      "SCOB\n",
      "SCOBU\n",
      "SCOBW\n",
      "SDGA\n",
      "SDGS\n",
      "SESN\n",
      "SFET\n",
      "SGHC.WS\n",
      "SHAC\n",
      "SHACU\n",
      "SHACW\n",
      "SHCA\n",
      "SHCAU\n",
      "SHCAW\n",
      "SHQA\n",
      "SHQAU\n",
      "SHQAW\n",
      "SIER\n",
      "SIERU\n",
      "SIERW\n",
      "SJI\n",
      "SJIJ\n",
      "SJIV\n",
      "SLAC.U\n",
      "SLAC.WS\n",
      "SMFR\n",
      "SMFRW\n",
      "SMIH\n",
      "SMIHU\n",
      "SMIHW\n",
      "SMTS\n",
      "SNUG\n",
      "SPGS\n",
      "SPGS.U\n",
      "SPGS.WS\n",
      "SPIR.WS\n",
      "SPK\n",
      "SPKAR\n",
      "SPKAU\n",
      "SPNE\n",
      "SPTK\n",
      "SPTKU\n",
      "SPTKW\n",
      "SRNE\n",
      "SSAA\n",
      "SSAAU\n",
      "SSAAW\n",
      "STON\n",
      "STOR\n",
      "STRN\n",
      "STRNW\n",
      "STRY\n",
      "STRY.WS\n",
      "STZ.B\n",
      "SVFA\n",
      "SVFAU\n",
      "SVFAW\n",
      "SVFB\n",
      "SWCH\n",
      "SWET\n",
      "SWETU\n",
      "SWETW\n",
      "SWIR\n",
      "SWT\n",
      "TBK\n",
      "TBKCP\n",
      "TBSA\n",
      "TBSAU\n",
      "TBSAW\n",
      "TCDA\n",
      "TECTP\n",
      "TEN\n",
      "THAC\n",
      "THACU\n",
      "THACW\n",
      "THCA\n",
      "THCAU\n",
      "THCAW\n",
      "TINV\n",
      "TINV.U\n",
      "TINV.WS\n",
      "TLGA.WS\n",
      "TMAC\n",
      "TMAC.U\n",
      "TMDI\n",
      "TPBA\n",
      "TPBAU\n",
      "TPBAW\n",
      "TRQ\n",
      "TSIB\n",
      "TSIBU\n",
      "TSIBW\n",
      "TSPQ\n",
      "TSPQ.U\n",
      "TSPQ.WS\n",
      "TTM\n",
      "TUEM\n",
      "TWND\n",
      "TWND.U\n",
      "TWND.WS\n",
      "TWNI.WS\n",
      "TYNE\n",
      "TZPS\n",
      "TZPSU\n",
      "TZPSW\n",
      "UBCB\n",
      "UMPQ\n",
      "USER\n",
      "USI\n",
      "VCKA\n",
      "VCKAU\n",
      "VCKAW\n",
      "VELO\n",
      "VELOU\n",
      "VELOW\n",
      "VENA\n",
      "VENAR\n",
      "VENAU\n",
      "VENAW\n",
      "VFLQ\n",
      "VGFC\n",
      "VIVO\n",
      "VLDR\n",
      "VLDRW\n",
      "VLNS\n",
      "VRGFF\n",
      "VSPY\n",
      "VTIQ\n",
      "VTIQU\n",
      "VTIQW\n",
      "VVNT\n",
      "WBEV\n",
      "WEBR\n",
      "WETF\n",
      "WIL\n",
      "WPCA\n",
      "WPCA.U\n",
      "WPCA.WS\n",
      "WPCB\n",
      "WPCB.U\n",
      "WPCB.WS\n",
      "WQGA\n",
      "WQGA.U\n",
      "WQGA.WS\n",
      "WTRH\n",
      "XL\n",
      "YMTX\n",
      "ZEN\n",
      "ZGN.WS\n",
      "ZNH\n",
      "ZRSEF\n",
      "ZTST\n",
      "ZVO\n",
      "ZWRK\n",
      "ZWRKU\n",
      "ZWRKW\n",
      "ZXYZ.A\n"
     ]
    }
   ],
   "source": [
    "# Loop through ticker_info documents and extract CIK and Composite FIGI for each ticker\n",
    "for doc in ticker_info_ref.stream():\n",
    "    ticker = doc.id\n",
    "    \n",
    "    # Check if \"is_test\" field is present and skip if it is true\n",
    "    if doc.to_dict().get('is_test') and doc.to_dict().get('is_test') == True:\n",
    "        continue\n",
    "    \n",
    "    value_str = \"\"\n",
    "    if 'cik' in doc.to_dict():\n",
    "        cik = doc.get('cik')\n",
    "        value_str+=\"cik\"+str(cik)\n",
    "    if 'composite_figi' in doc.to_dict():\n",
    "        composite_figi = doc.get('composite_figi')\n",
    "        value_str+=\"compfigi\"+str(composite_figi)\n",
    "    if value_str == \"\":\n",
    "        print(ticker)\n",
    "        continue\n",
    "    # Add ticker and value string to ticker_us_equity_map dictionary\n",
    "    ticker_us_equity_map[ticker] = value_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeddcba0-f55f-4138-b951-303ebeb32698",
   "metadata": {},
   "source": [
    "user data feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08248319-1644-4264-85b0-f966415d1b68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ticker_vector(user_pref, ticker_map_dict, ticker_sic_map):\n",
    "    ticker_vector = [0] * len(ticker_map_dict)\n",
    "    \n",
    "    for ticker, industry in ticker_sic_map.items():\n",
    "        if industry is None:\n",
    "            continue\n",
    "        if industry in user_pref and user_pref[industry]:\n",
    "            ticker_vector[ticker_map_dict[ticker]] += 1\n",
    "        elif industry in user_pref and not user_pref[industry]:\n",
    "            ticker_vector[ticker_map_dict[ticker]] -= 1\n",
    "    \n",
    "    return ticker_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9396bc9a-7af5-4b70-9fa0-b6e18b575fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all documents from the user_preference collection\n",
    "user_pref_docs = db.collection('user_preference').stream()\n",
    "\n",
    "# Initialize an empty dictionary to store user preferences for each ticker\n",
    "user_ticker_pref = {}\n",
    "\n",
    "# Iterate through each user preference document\n",
    "for doc in user_pref_docs:\n",
    "    # Get the user preference dictionary from the document data\n",
    "    user_pref = doc.to_dict()\n",
    "    \n",
    "    # Initialize a list to store the ticker vector for this user\n",
    "    ticker_vector = [0] * len(ticker_map_dict)\n",
    "    \n",
    "    # Iterate through each ticker in the ticker_sic_map\n",
    "    for ticker, industry in ticker_sic_map.items():\n",
    "        # Get the index of this ticker in the ticker_vector using the ticker_map_dict\n",
    "        ticker_index = ticker_map_dict[ticker]\n",
    "        \n",
    "        # Check if the industry for this ticker matches any industry in the user preference\n",
    "        for pref_industry, pref_value in user_pref.items():\n",
    "            if industry == pref_industry and pref_value:\n",
    "                # Increase the value of this ticker's index in the ticker_vector\n",
    "                ticker_vector[ticker_index] += 1\n",
    "            elif industry == pref_industry and not pref_value:\n",
    "                # Decrease the value of this ticker's index in the ticker_vector\n",
    "                ticker_vector[ticker_index] -= 1\n",
    "        \n",
    "    # Add this user's ticker vector to the user_ticker_pref dictionary\n",
    "    user_ticker_pref[doc.id] = ticker_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2b2322-242e-49eb-95ad-d8e6500e1a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all documents from the user_preference collection\n",
    "user_pref_docs = db.collection('user_preference').stream()\n",
    "\n",
    "# Initialize an empty dictionary to store user preferences for each ticker\n",
    "user_ticker_pref = {}\n",
    "\n",
    "# Iterate through each user preference document\n",
    "for doc in user_pref_docs:\n",
    "    # Get the user preference dictionary from the document data\n",
    "    user_pref = doc.to_dict()\n",
    "    \n",
    "    # Initialize a list to store the ticker vector for this user\n",
    "    ticker_vector = [0] * len(ticker_map_dict)\n",
    "    \n",
    "    # Iterate through each ticker in the ticker_sic_map\n",
    "    for ticker, industry in ticker_sic_map.items():\n",
    "        # Get the index of this ticker in the ticker_vector using the ticker_map_dict\n",
    "        ticker_index = ticker_map_dict[ticker]\n",
    "        \n",
    "        # Check if the industry for this ticker matches any industry in the user preference\n",
    "        for pref_industry, pref_value in user_pref.items():\n",
    "            if industry == pref_industry and pref_value:\n",
    "                # Increase the value of this ticker's index in the ticker_vector\n",
    "                ticker_vector[ticker_index] += 1\n",
    "            elif industry == pref_industry and not pref_value:\n",
    "                # Decrease the value of this ticker's index in the ticker_vector\n",
    "                ticker_vector[ticker_index] -= 1\n",
    "        \n",
    "    # Add this user's ticker vector to the user_ticker_pref dictionary\n",
    "    user_ticker_pref[doc.id] = ticker_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ee3a7-ea96-4023-bf35-1a650111d332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all documents from the user_preference collection\n",
    "user_pref_docs = db.collection('user_preferences').stream()\n",
    "\n",
    "# Initialize an empty dictionary to store user preferences for each ticker\n",
    "user_ticker_pref = {}\n",
    "\n",
    "# Iterate through each user preference document\n",
    "for doc in user_pref_docs:\n",
    "    # Get the user preference dictionary from the document data\n",
    "    user_pref = doc.to_dict()\n",
    "    \n",
    "    # Initialize a list to store the ticker vector for this user\n",
    "    ticker_vector = [0] * len(ticker_map_dict)\n",
    "    \n",
    "    # Iterate through each ticker in the ticker_sic_map\n",
    "    for ticker, industry in ticker_sic_map.items():\n",
    "        if ticker==\"nan\":\n",
    "            pass\n",
    "        else:\n",
    "            # Get the index of this ticker in the ticker_vector using the ticker_map_dict\n",
    "            if ticker not in ticker_map_dict.keys():\n",
    "                print(ticker)\n",
    "            ticker_index = ticker_map_dict[ticker]\n",
    "\n",
    "            # Check if the industry for this ticker matches any industry in the user preference\n",
    "            for pref_industry, pref_value in user_pref.items():\n",
    "                if industry == pref_industry and pref_value:\n",
    "                    # Increase the value of this ticker's index in the ticker_vector\n",
    "                    ticker_vector[ticker_index] += 1\n",
    "                elif industry == pref_industry and not pref_value:\n",
    "                    # Decrease the value of this ticker's index in the ticker_vector\n",
    "                    ticker_vector[ticker_index] -= 1\n",
    "        \n",
    "    # Add this user's ticker vector to the user_ticker_pref dictionary\n",
    "    user_ticker_pref[doc.id] = ticker_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce981a28-736d-423c-bf41-73855f6256ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate through the user_ticker_pref dictionary and set values in the collection\n",
    "for user_id, ticker_vector in user_ticker_pref.items():\n",
    "    # Set the document reference in the user_pref_ticker collection using the user_id\n",
    "    doc_ref = db.collection('user_pref_ticker').document(user_id)\n",
    "\n",
    "    # Add the ticker_vector to the document\n",
    "    doc_ref.set({'ticker_vector': ticker_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327e3ab-9977-4f4c-9f32-eb37c5563228",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[\"tickers_index\"] = news_data.tickers.apply(lambda tickers: [ticker_map_dict.get(ticker, None) for ticker in tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a613c84-b286-4f51-a645-611c9c269006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = user_ticker_pref[\"2qbeT9d3aCfrSk0PLbsJO10mrV73\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71e585-b051-46c8-947d-d1c08a47801a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_pref_news = list(map(lambda x: np.nanmean([user[i] for i in x if i is not None]), news_data['tickers_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa4c9c-4792-4ed0-b1ac-9656a373e8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_pref_docs = db.collection('user_preferences').stream()\n",
    "industry_news_ref = db.collection('industry_news').document(\"dict\")\n",
    "industry_news = industry_news_ref.get().to_dict()\n",
    "print(industry_news.keys())\n",
    "for doc in user_pref_docs:\n",
    "    # Get the user preference dictionary from the document data\n",
    "    user_pref = doc.to_dict()\n",
    "\n",
    "    # Extract the industry names for which the value is True\n",
    "    industry_names = [key for key, value in user_pref.items() if value]\n",
    "    print(industry_names)\n",
    "\n",
    "    # Create an empty dictionary to store the merged documents\n",
    "    merged_docs = {}\n",
    "\n",
    "    # Loop through each industry name and get the corresponding document from Firestore\n",
    "    for name in industry_names:\n",
    "        merged_docs.update(industry_news[name])\n",
    "\n",
    "    # Sort the merged documents by publish_utc in descending order\n",
    "    sorted_docs = dict(sorted(merged_docs.items(), key=lambda x: x[1]['publish_utc'], reverse=True))\n",
    "\n",
    "    # Get the 100 most recent rows from the sorted documents\n",
    "    most_recent_docs = dict(list(sorted_docs.items())[:5])\n",
    "    \n",
    "    print(most_recent_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c48d3-2ed1-4d13-be8e-bfbaa404ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_keys(user_preferences):\n",
    "    corrected_keys = {\n",
    "        'algriculture': 'agriculture',\n",
    "        'construction': 'construction',\n",
    "        'transportation': 'transportation',\n",
    "        'manufacuring': 'manufacturing',\n",
    "        'wholesale': 'wholesale',\n",
    "        'public_administration': 'public_administration',\n",
    "        'mining': 'mining',\n",
    "        'finance': 'finance',\n",
    "        'retail': 'retail',\n",
    "        'services': 'services'\n",
    "    }\n",
    "\n",
    "    return {corrected_keys[key]: value for key, value in user_preferences.items()}\n",
    "\n",
    "def correct_user_pref_key():\n",
    "    \"\"\"\n",
    "    no need to call it in the future, one time fix of user pref collection's dict keys\n",
    "    \"\"\"\n",
    "    user_preferences_ref = db.collection('user_preferences')\n",
    "    for doc in user_preferences_ref.stream():\n",
    "        user_preferences = doc.to_dict()\n",
    "        corrected_preferences = rename_keys(user_preferences)\n",
    "        # Update the document with the corrected key-value pairs\n",
    "        doc_ref = user_preferences_ref.document(doc.id)\n",
    "        doc_ref.set(corrected_preferences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
