{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "76deafa2-c5ec-4a2d-8fa0-cb0a49de9b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyarrow import feather\n",
    "from scipy import sparse\n",
    "from queue import Queue\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "#import concurrent\n",
    "import multiprocessing\n",
    "from firebase_helper import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50006515-c613-4b87-bc50-734ad2a1cb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fb = firebase_helper()\n",
    "db = fb.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dae6ffd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.firestore_v1.client.Client at 0x7f2e10c9fc70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de023d6c-e68f-4ad0-87eb-414520854d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ticker_map_path = \"/mnt/d/data/news/ticker_maping_dict.pkl\"\n",
    "ticker_map_dict = pickle.load(open(ticker_map_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4804a0a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#overwrite ticker mapping on db\n",
    "doc_ref = db.collection('ticker_map').document('dict')\n",
    "doc_ref.set(ticker_map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c985c483",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "knn_ref = db.collection(\"knn\").document(\"knn_ticker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e49077-8e82-4b45-a1c0-60bb22fb17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_data = np.load(\"knn_ticker.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0336b19f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'knn_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         insert_data(batch, i)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmin\u001b[39m(max_cpus, os\u001b[38;5;241m.\u001b[39mcpu_count())) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mknn_data\u001b[49m), batch_size)):\n\u001b[1;32m     23\u001b[0m         batch \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mbatch()\n\u001b[1;32m     24\u001b[0m         process_batch(batch, \u001b[38;5;28mrange\u001b[39m(i, \u001b[38;5;28mmin\u001b[39m(i \u001b[38;5;241m+\u001b[39m batch_size, \u001b[38;5;28mlen\u001b[39m(knn_data))))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'knn_data' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import os\n",
    "from google.cloud.firestore_v1 import batch\n",
    "from google.cloud.firestore_v1.batch import WriteBatch\n",
    "from google.cloud.firestore_v1.client import Client\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_cpus = 12  # Maximum number of CPUs to use\n",
    "batch_size = 500  # Batch size for Firestore writes\n",
    "\n",
    "def insert_data(batch: batch, i: int) -> None:\n",
    "    lc = knn_data[i]\n",
    "    temp_map = {str(j): lc[j] for j in range(len(lc)) if lc[j] != 0}\n",
    "    batch.set(db.collection('knn_news').document(str(i)), temp_map)\n",
    "\n",
    "def process_batch(batch: WriteBatch, indices: List[int]) -> None:\n",
    "    for i in indices:\n",
    "        insert_data(batch, i)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=min(max_cpus, os.cpu_count())) as executor:\n",
    "    for i in tqdm(range(0, len(knn_data), batch_size)):\n",
    "        batch = db.batch()\n",
    "        process_batch(batch, range(i, min(i + batch_size, len(knn_data))))\n",
    "        batch.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf41a67b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "n = len(knn_data)\n",
    "base_user_pref_vector = np.zeros(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fe1680b-bb65-41fc-9798-cb34d9d0ef8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def memoize(function):\n",
    "    cache = {}\n",
    "    def wrapper(input):\n",
    "        if input not in cache:\n",
    "            cache[input] = function(input)\n",
    "        return cache[input]\n",
    "    return wrapper\n",
    "\n",
    "@memoize\n",
    "def sic_match(input):\n",
    "    sic_codes = {\n",
    "        '01': 'agriculture',\n",
    "        '02': 'agriculture',\n",
    "        '07': 'agriculture',\n",
    "        '08': 'agriculture',\n",
    "        '09': 'agriculture',\n",
    "        '10': 'mining',\n",
    "        '11': 'mining',\n",
    "        '12': 'mining',\n",
    "        '13': 'mining',\n",
    "        '14': 'mining',\n",
    "        '15': 'construction',\n",
    "        '16': 'construction',\n",
    "        '17': 'construction',\n",
    "        **{f\"{i:02d}\": \"manufacturing\" for i in range(20, 40)},\n",
    "        **{f\"{i:02d}\": \"transportation\" for i in range(40, 50)},\n",
    "        '50': 'wholesale',\n",
    "        '51': 'wholesale',\n",
    "        **{f\"{i:02d}\": \"retail\" for i in range(52, 60)},\n",
    "        **{f\"{i:02d}\": \"finance\" for i in range(60, 68)},\n",
    "        **{f\"{i:02d}\": \"services\" for i in range(70, 90)},\n",
    "        **{f\"{i:02d}\": \"public_administration\" for i in range(91, 100)},\n",
    "    }\n",
    "    try:\n",
    "        return sic_codes[input]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid input. Please enter a two-character string matching a valid SIC code.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b23b073a-d8b2-4b7b-8000-2ece121b5046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agriculture\n"
     ]
    }
   ],
   "source": [
    "print(sic_match(\"01\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71585e52-ec26-4b29-98f5-575c2dc21fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#base imports\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "poly_dir = os.path.abspath(os.path.join(os.getcwd(), 'data_poly'))\n",
    "sys.path.append(poly_dir)\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import itertools\n",
    "\n",
    "# package imports\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "# local imports\n",
    "import data_poly.poly_getdata as poly_getdata\n",
    "import data_poly.poly_url as poly_url\n",
    "import data_poly.poly_helper as poly_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "742c31d3-2ded-4a63-bf83-966395e88c13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"/home/peterzerg/repos/PeterZergQuant/.ENV\")\n",
    "api_key = os.environ.get(\"POLYGON_APIKEY_MASTER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "deb7f1db-05cd-497e-9a31-ea0cd63a56d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_factory = poly_url.StockUrlFactory(api_key)\n",
    "ticker_lc = ticker_map_dict.keys()\n",
    "urls_dict = {ticker: url_factory.ReferenceData.ticker_info(url_factory, ticker) for ticker in ticker_lc}\n",
    "df_dict = await poly_helper.get_data_from_urls(urls_dict)\n",
    "df = pd.concat(df_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "914882c3-fda0-4396-b191-aa84d5cd4928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "upsert_dict = {ticker: df.to_dict('records')[0] for ticker, df in df_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556eaccc-f6f9-444e-93f4-4567982e5e39",
   "metadata": {},
   "source": [
    "upload ticker info to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e01d5ca5-9569-4886-8daf-c57c3e9265a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunks(data, size):\n",
    "    data_keys = list(data.keys())\n",
    "    for i in range(0, len(data_keys), size):\n",
    "        yield {k: data[k] for k in data_keys[i:i+size]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "898152ae-77ac-4d57-ab6c-5ba3a6833fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the reference to the ticker_info collection\n",
    "collection_ref = db.collection('ticker_info')\n",
    "\n",
    "# Split upsert_dict into chunks with a maximum size of 500\n",
    "chunk_size = 500\n",
    "sub_dicts = list(chunks(upsert_dict, chunk_size))\n",
    "\n",
    "# Iterate through the sub_dicts\n",
    "for sub_dict in sub_dicts:\n",
    "    # Create a batch to batch the writes\n",
    "    batch = db.batch()\n",
    "    \n",
    "    # Iterate through the sub_dict\n",
    "    for key, value in sub_dict.items():\n",
    "        # Add the set operation to the batch\n",
    "        doc_ref = collection_ref.document(key)\n",
    "        batch.set(doc_ref, value)\n",
    "    \n",
    "    # Commit the batch\n",
    "    batch.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bfc88-6efb-4929-bef1-c56f8cdd75e4",
   "metadata": {},
   "source": [
    "upload ticker sic mapping to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5608f520-781a-4539-b8d8-f9b03f9ecd91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sic_map_dict = {\n",
    "    str(ticker): sic_match(sic_code[:2]) if isinstance(sic_code, str) else None\n",
    "    for ticker, sic_code in zip(df.ticker, df.sic_code)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6abadeda-91ad-4b2c-ac2c-f04534a64166",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_time {\n",
       "  seconds: 1679430267\n",
       "  nanos: 25464000\n",
       "}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload data to db\n",
    "doc_ref = db.collection('ticker_sic_map').document('dict')\n",
    "doc_ref.set(sic_map_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21cda6f-3c62-4510-b8f9-76bb252d0211",
   "metadata": {
    "tags": []
   },
   "source": [
    "upload news data to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8d3c5f99-b259-4d66-9c77-f74e7abfac3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "news_data = pd.read_feather(\"/mnt/d/data/news/local_us_equity_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "27bfe880-a0a5-4b6c-84f1-0ee9ac8e56cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get reference to the document\n",
    "doc_ref = db.collection('ticker_sic_map').document('dict')\n",
    "\n",
    "# Retrieve the document data\n",
    "doc = doc_ref.get()\n",
    "\n",
    "# Check if the document exists\n",
    "if doc.exists:\n",
    "    # Get the document data\n",
    "    ticker_sic_map = doc.to_dict()\n",
    "else:\n",
    "    print(f\"No such document: {doc_ref.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "115eb9d3-555f-4d7f-a871-9e07b7fbee9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#add 10 industry cols\n",
    "news_data[\"industries\"] = news_data.tickers.apply(lambda tickers: [ticker_sic_map.get(ticker, None) for ticker in tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ab046a72-0e91-435d-abfa-00eacfa7c37c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 819163/819163 [00:00<00:00, 6138130.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the industry column names and default values\n",
    "industry_cols = list(set(['agriculture', 'mining', 'construction', 'manufacturing', 'transportation',\n",
    "                 'wholesale', 'retail', 'finance', 'services', 'public_administration']))\n",
    "default_value = False\n",
    "\n",
    "# Define a function to convert the industries list to a boolean list\n",
    "def to_boolean_list(industries):\n",
    "    return [col in industries for col in industry_cols]\n",
    "\n",
    "# Create a dataframe with the boolean values for each industry\n",
    "boolean_df = pd.DataFrame(tqdm(news_data['industries'].apply(to_boolean_list).tolist()), columns=industry_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "096d1197-0a17-4821-a56b-17a8b7edc6c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news_data = news_data.reset_index(drop=True)\n",
    "boolean_df = boolean_df.reset_index(drop=True)\n",
    "news_data = pd.concat([news_data, boolean_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "52eb4a2b-7cf5-48d1-b60b-273380da9716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_817/279113026.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  news_data_dict = news_data.set_index('id').T.to_dict()\n"
     ]
    }
   ],
   "source": [
    "# Convert arrays to lists\n",
    "def convert_arrays_to_lists(value):\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return list(value)\n",
    "    return value\n",
    "\n",
    "news_data = news_data.applymap(convert_arrays_to_lists)\n",
    "\n",
    "# Convert the DataFrame to a dictionary format\n",
    "news_data_dict = news_data.set_index('id').T.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7e4e4622-cae3-4a24-8b7c-80ec3eea68f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 617/617 [32:20<00:00,  3.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# Split the news_data_dict into chunks of 500\n",
    "chunk_size = 500\n",
    "sub_dicts = list(chunks(news_data_dict, chunk_size))\n",
    "\n",
    "# Iterate through the sub_dicts\n",
    "for sub_dict in tqdm(sub_dicts):\n",
    "    # Create a batch to batch the writes\n",
    "    batch = db.batch()\n",
    "\n",
    "    # Iterate through the key-value pairs in the sub_dict\n",
    "    for key, value in sub_dict.items():\n",
    "        # Set the document reference in the news_data collection using the key\n",
    "        doc_ref = db.collection('news_data').document(str(key))\n",
    "        \n",
    "        # Add the key-value pair to the batch\n",
    "        batch.set(doc_ref, value)\n",
    "    \n",
    "    # Commit the batch\n",
    "    batch.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1c366975-91e3-4420-b8fe-34a84e71b698",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#incase we want to use async\n",
    "import asyncio\n",
    "from google.cloud import firestore\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the Firestore client\n",
    "db = firestore.Client()\n",
    "\n",
    "# Define a coroutine for writing a document to Firestore\n",
    "async def write_document(doc_ref, data):\n",
    "    await doc_ref.set(data)\n",
    "\n",
    "# Define a function to write a batch of documents to Firestore asynchronously\n",
    "async def write_batch(batch):\n",
    "    # Create a semaphore to limit the number of concurrent tasks\n",
    "    semaphore = asyncio.Semaphore(12)\n",
    "\n",
    "    # Create a list to hold the coroutines\n",
    "    coroutines = []\n",
    "\n",
    "    # Iterate through the batch and create a coroutine for each document\n",
    "    for key, value in batch.items():\n",
    "        # Set the document reference in the news_data collection using the key\n",
    "        doc_ref = db.collection('news_data').document(str(key))\n",
    "\n",
    "        # Create a coroutine to write the document to Firestore\n",
    "        coroutine = write_document(doc_ref, value)\n",
    "\n",
    "        # Append the coroutine to the list\n",
    "        coroutines.append(coroutine)\n",
    "\n",
    "    # Run the coroutines concurrently with the semaphore\n",
    "    async with semaphore:\n",
    "        await asyncio.gather(*coroutines)\n",
    "\n",
    "# Split the news_data_dict into chunks of 500\n",
    "chunk_size = 500\n",
    "sub_dicts = list(chunks(news_data_dict, chunk_size))\n",
    "\n",
    "# Iterate through the sub_dicts\n",
    "for sub_dict in tqdm(sub_dicts):\n",
    "    # Create a batch to batch the writes\n",
    "    batch = {}\n",
    "\n",
    "    # Iterate through the key-value pairs in the sub_dict\n",
    "    for key, value in sub_dict.items():\n",
    "        # Add the key-value pair to the batch\n",
    "        batch[key] = value\n",
    "\n",
    "    # Run the write_batch coroutine asynchronously\n",
    "    asyncio.run(write_batch(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c6729e2-ce65-4673-aa64-1da614b4afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map ticker to industry (OK)\n",
    "#get tickers col as list from master news dataframe (rows) (OK)\n",
    "#map tickers in list 10 industries boolean col (OK)\n",
    "#create 10 industry dataframes, 100 rows each, order by time (OK)\n",
    "#write a python program that run every x mins to get new news and append to the master dataframe, and also append to the 10 industries dataframes.\n",
    "#user requests for news -> look up pref in firebase by user hashid -> merge industries dataframes -> push to front end 100 rows but limit displace 10 rows at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8468505a-3336-436e-8b3b-2e78a7ca6c04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_817/254609390.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  industry_data[col] = industry_rows.set_index('id').T.to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "services: 100 rows added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_817/254609390.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  industry_data[col] = industry_rows.set_index('id').T.to_dict()\n",
      "/tmp/ipykernel_817/254609390.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  industry_data[col] = industry_rows.set_index('id').T.to_dict()\n",
      "/tmp/ipykernel_817/254609390.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  industry_data[col] = industry_rows.set_index('id').T.to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manufacturing: 100 rows added\n",
      "construction: 100 rows added\n",
      "wholesale: 100 rows added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_817/254609390.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  industry_data[col] = industry_rows.set_index('id').T.to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transportation: 100 rows added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_817/254609390.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  industry_data[col] = industry_rows.set_index('id').T.to_dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finance: 100 rows added\n",
      "retail: 100 rows added\n",
      "public_administration: 0 rows added\n",
      "mining: 100 rows added\n",
      "agriculture: 100 rows added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_817/254609390.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  industry_data[col] = industry_rows.set_index('id').T.to_dict()\n",
      "/tmp/ipykernel_817/254609390.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  industry_data[col] = industry_rows.set_index('id').T.to_dict()\n",
      "/tmp/ipykernel_817/254609390.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  industry_data[col] = industry_rows.set_index('id').T.to_dict()\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to hold the smaller dataframes\n",
    "industry_data = {}\n",
    "\n",
    "# Iterate through the industry columns\n",
    "for col in industry_cols:\n",
    "    # Get the most recent 100 rows where the industry boolean column is true\n",
    "    industry_rows = news_data[news_data[col] == True].sort_values(by='published_utc', ascending=False).head(100)\n",
    "    \n",
    "    # Add the dataframe to the dictionary using the column name as the key\n",
    "    industry_data[col] = industry_rows.set_index('id').T.to_dict()\n",
    "\n",
    "    # Print the number of rows added to the dataframe\n",
    "    print(f\"{col}: {len(industry_rows)} rows added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e59211f0-1cbc-4c11-97fb-0af9c7c1d591",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_time {\n",
       "  seconds: 1679437164\n",
       "  nanos: 283412000\n",
       "}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload 100 news industries data to db\n",
    "doc_ref = db.collection('industry_news').document('dict')\n",
    "doc_ref.set(industry_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeddcba0-f55f-4138-b951-303ebeb32698",
   "metadata": {},
   "source": [
    "user data feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "08248319-1644-4264-85b0-f966415d1b68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ticker_vector(user_pref, ticker_map_dict, ticker_sic_map):\n",
    "    ticker_vector = [0] * len(ticker_map_dict)\n",
    "    \n",
    "    for ticker, industry in ticker_sic_map.items():\n",
    "        if industry is None:\n",
    "            continue\n",
    "        if industry in user_pref and user_pref[industry]:\n",
    "            ticker_vector[ticker_map_dict[ticker]] += 1\n",
    "        elif industry in user_pref and not user_pref[industry]:\n",
    "            ticker_vector[ticker_map_dict[ticker]] -= 1\n",
    "    \n",
    "    return ticker_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9396bc9a-7af5-4b70-9fa0-b6e18b575fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all documents from the user_preference collection\n",
    "user_pref_docs = db.collection('user_preference').stream()\n",
    "\n",
    "# Initialize an empty dictionary to store user preferences for each ticker\n",
    "user_ticker_pref = {}\n",
    "\n",
    "# Iterate through each user preference document\n",
    "for doc in user_pref_docs:\n",
    "    # Get the user preference dictionary from the document data\n",
    "    user_pref = doc.to_dict()\n",
    "    \n",
    "    # Initialize a list to store the ticker vector for this user\n",
    "    ticker_vector = [0] * len(ticker_map_dict)\n",
    "    \n",
    "    # Iterate through each ticker in the ticker_sic_map\n",
    "    for ticker, industry in ticker_sic_map.items():\n",
    "        # Get the index of this ticker in the ticker_vector using the ticker_map_dict\n",
    "        ticker_index = ticker_map_dict[ticker]\n",
    "        \n",
    "        # Check if the industry for this ticker matches any industry in the user preference\n",
    "        for pref_industry, pref_value in user_pref.items():\n",
    "            if industry == pref_industry and pref_value:\n",
    "                # Increase the value of this ticker's index in the ticker_vector\n",
    "                ticker_vector[ticker_index] += 1\n",
    "            elif industry == pref_industry and not pref_value:\n",
    "                # Decrease the value of this ticker's index in the ticker_vector\n",
    "                ticker_vector[ticker_index] -= 1\n",
    "        \n",
    "    # Add this user's ticker vector to the user_ticker_pref dictionary\n",
    "    user_ticker_pref[doc.id] = ticker_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d2b2322-242e-49eb-95ad-d8e6500e1a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all documents from the user_preference collection\n",
    "user_pref_docs = db.collection('user_preference').stream()\n",
    "\n",
    "# Initialize an empty dictionary to store user preferences for each ticker\n",
    "user_ticker_pref = {}\n",
    "\n",
    "# Iterate through each user preference document\n",
    "for doc in user_pref_docs:\n",
    "    # Get the user preference dictionary from the document data\n",
    "    user_pref = doc.to_dict()\n",
    "    \n",
    "    # Initialize a list to store the ticker vector for this user\n",
    "    ticker_vector = [0] * len(ticker_map_dict)\n",
    "    \n",
    "    # Iterate through each ticker in the ticker_sic_map\n",
    "    for ticker, industry in ticker_sic_map.items():\n",
    "        # Get the index of this ticker in the ticker_vector using the ticker_map_dict\n",
    "        ticker_index = ticker_map_dict[ticker]\n",
    "        \n",
    "        # Check if the industry for this ticker matches any industry in the user preference\n",
    "        for pref_industry, pref_value in user_pref.items():\n",
    "            if industry == pref_industry and pref_value:\n",
    "                # Increase the value of this ticker's index in the ticker_vector\n",
    "                ticker_vector[ticker_index] += 1\n",
    "            elif industry == pref_industry and not pref_value:\n",
    "                # Decrease the value of this ticker's index in the ticker_vector\n",
    "                ticker_vector[ticker_index] -= 1\n",
    "        \n",
    "    # Add this user's ticker vector to the user_ticker_pref dictionary\n",
    "    user_ticker_pref[doc.id] = ticker_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8a8ee3a7-ea96-4023-bf35-1a650111d332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all documents from the user_preference collection\n",
    "user_pref_docs = db.collection('user_preferences').stream()\n",
    "\n",
    "# Initialize an empty dictionary to store user preferences for each ticker\n",
    "user_ticker_pref = {}\n",
    "\n",
    "# Iterate through each user preference document\n",
    "for doc in user_pref_docs:\n",
    "    # Get the user preference dictionary from the document data\n",
    "    user_pref = doc.to_dict()\n",
    "    \n",
    "    # Initialize a list to store the ticker vector for this user\n",
    "    ticker_vector = [0] * len(ticker_map_dict)\n",
    "    \n",
    "    # Iterate through each ticker in the ticker_sic_map\n",
    "    for ticker, industry in ticker_sic_map.items():\n",
    "        if ticker==\"nan\":\n",
    "            pass\n",
    "        else:\n",
    "            # Get the index of this ticker in the ticker_vector using the ticker_map_dict\n",
    "            if ticker not in ticker_map_dict.keys():\n",
    "                print(ticker)\n",
    "            ticker_index = ticker_map_dict[ticker]\n",
    "\n",
    "            # Check if the industry for this ticker matches any industry in the user preference\n",
    "            for pref_industry, pref_value in user_pref.items():\n",
    "                if industry == pref_industry and pref_value:\n",
    "                    # Increase the value of this ticker's index in the ticker_vector\n",
    "                    ticker_vector[ticker_index] += 1\n",
    "                elif industry == pref_industry and not pref_value:\n",
    "                    # Decrease the value of this ticker's index in the ticker_vector\n",
    "                    ticker_vector[ticker_index] -= 1\n",
    "        \n",
    "    # Add this user's ticker vector to the user_ticker_pref dictionary\n",
    "    user_ticker_pref[doc.id] = ticker_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce981a28-736d-423c-bf41-73855f6256ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate through the user_ticker_pref dictionary and set values in the collection\n",
    "for user_id, ticker_vector in user_ticker_pref.items():\n",
    "    # Set the document reference in the user_pref_ticker collection using the user_id\n",
    "    doc_ref = db.collection('user_pref_ticker').document(user_id)\n",
    "\n",
    "    # Add the ticker_vector to the document\n",
    "    doc_ref.set({'ticker_vector': ticker_vector})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327e3ab-9977-4f4c-9f32-eb37c5563228",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data[\"tickers_index\"] = news_data.tickers.apply(lambda tickers: [ticker_map_dict.get(ticker, None) for ticker in tickers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7a613c84-b286-4f51-a645-611c9c269006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = user_ticker_pref[\"2qbeT9d3aCfrSk0PLbsJO10mrV73\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1c71e585-b051-46c8-947d-d1c08a47801a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_pref_news = list(map(lambda x: np.nanmean([user[i] for i in x if i is not None]), news_data['tickers_index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dbfa4c9c-4792-4ed0-b1ac-9656a373e8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['construction', 'agriculture', 'transportation', 'manufacturing', 'wholesale', 'public_administration', 'mining', 'finance', 'retail', 'services'])\n",
      "['manufacuring', 'construction', 'transportation', 'algriculture', 'public_administration', 'mining', 'wholesale', 'finance', 'retail', 'services']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'manufacuring'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Loop through each industry name and get the corresponding document from Firestore\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m industry_names:\n\u001b[0;32m---> 18\u001b[0m     merged_docs\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mindustry_news\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Sort the merged documents by publish_utc in descending order\u001b[39;00m\n\u001b[1;32m     21\u001b[0m sorted_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28msorted\u001b[39m(merged_docs\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublish_utc\u001b[39m\u001b[38;5;124m'\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'manufacuring'"
     ]
    }
   ],
   "source": [
    "user_pref_docs = db.collection('user_preferences').stream()\n",
    "industry_news_ref = db.collection('industry_news').document(\"dict\")\n",
    "industry_news = industry_news_ref.get().to_dict()\n",
    "print(industry_news.keys())\n",
    "for doc in user_pref_docs:\n",
    "    # Get the user preference dictionary from the document data\n",
    "    user_pref = doc.to_dict()\n",
    "\n",
    "    # Extract the industry names for which the value is True\n",
    "    industry_names = [key for key, value in user_pref.items() if value]\n",
    "    print(industry_names)\n",
    "\n",
    "    # Create an empty dictionary to store the merged documents\n",
    "    merged_docs = {}\n",
    "\n",
    "    # Loop through each industry name and get the corresponding document from Firestore\n",
    "    for name in industry_names:\n",
    "        merged_docs.update(industry_news[name])\n",
    "\n",
    "    # Sort the merged documents by publish_utc in descending order\n",
    "    sorted_docs = dict(sorted(merged_docs.items(), key=lambda x: x[1]['publish_utc'], reverse=True))\n",
    "\n",
    "    # Get the 100 most recent rows from the sorted documents\n",
    "    most_recent_docs = dict(list(sorted_docs.items())[:5])\n",
    "    \n",
    "    print(most_recent_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c48d3-2ed1-4d13-be8e-bfbaa404ee9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
