{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04983bda-2d74-4989-a41b-5cbac5bf16f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#base imports\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "poly_dir = os.path.abspath(os.path.join(os.getcwd(), 'data_poly'))\n",
    "sys.path.append(poly_dir)\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import itertools\n",
    "import concurrent.futures\n",
    "\n",
    "# package imports\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow.feather as feather\n",
    "import pickle\n",
    "from firebase_helper import *\n",
    "import math\n",
    "import asyncio\n",
    "from pandas.io.json import json_normalize\n",
    "import hashlib\n",
    "from google.cloud.firestore_v1 import Query\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from IPython.display import display\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "# local imports\n",
    "import data_poly.poly_getdata as poly_getdata\n",
    "import data_poly.poly_url as poly_url\n",
    "import data_poly.poly_helper as poly_helper\n",
    "\n",
    "\n",
    "# Suppress the UserWarning with a specific message\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"DataFrame columns are not unique, some columns will be omitted.\",\n",
    "    category=UserWarning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049fd872-c9aa-4492-9343-ad76abd748ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"/home/peterzerg/repos/quant/.ENV\")\n",
    "api_key = os.environ.get(\"POLYGON_APIKEY_MASTER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38014afa-a01c-40e2-aaec-15ecdd970f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fb = firebase_helper()\n",
    "db = fb.get_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50006515-c613-4b87-bc50-734ad2a1cb3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#project global vars\n",
    "industry_cols= ['agriculture', 'mining', 'construction', 'manufacturing', 'transportation','wholesale', 'retail', 'finance', 'services', 'public_administration']\n",
    "object_names = ['ticker_hash_dict','hash_sic_dict','news_data_dict','knn_dict','user_ticker_pref_dict','ticker_info_dict','industry_news_dict','industry_news_compiled_dict',\n",
    "                'news_datetime_dict','industry_news_hash_dict','preference_scores_user_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aca51b-fe1e-4a20-a526-926c91921bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save local\n",
    "# Loop over the object names\n",
    "for name in tqdm(object_names):\n",
    "    # Load the object by name\n",
    "    obj = globals()[name]\n",
    "    # Save the object as a pickle file using the name\n",
    "    with open(f'{name}.pickle', 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77866cf3-a249-4ec0-a68d-3b7bee9f6298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:10<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "#read local\n",
    "# Loop over the object names\n",
    "for name in tqdm(object_names):\n",
    "    # Load the object by name\n",
    "    with open(f'{name}.pickle', 'rb') as f:\n",
    "        globals()[name] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62c63366-6c72-400f-9388-4bf239e0b60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#frequent updates:\n",
    "async def frequent_update():\n",
    "    global news_data_dict, industry_news_dict, user_ticker_pref_dict, industry_news_compiled_dict, news_datetime_dict, industry_news_hash_dict, preference_scores_user_rank\n",
    "    #gen updated news\n",
    "    news_data_dict = await run_news_update()\n",
    "    #gen industry news\n",
    "    industry_news_dict = gen_10_industries_df()\n",
    "    #gen user_ticker_pref\n",
    "    user_ticker_pref_dict = user_base_pref_gen()\n",
    "    industry_news_compiled_dict, news_datetime_dict = get_industry_news()\n",
    "    industry_news_hash_dict = industry_news_to_hash()\n",
    "    #gen user_news_rank\n",
    "    preference_scores_user_rank = get_user_news_rank()\n",
    "    #uploads\n",
    "    await upload_industry_data()\n",
    "    await upload_user_ticker_pref()\n",
    "    await upload_preference_scores_user_rank()\n",
    "    upload_time_tracker()\n",
    "    \n",
    "async def upload_industry_data():\n",
    "    global db, industry_news_dict\n",
    "    await cloud_upload(db, industry_news_dict, \"industry_data\")\n",
    "    total_news_dict = flatten_dict(industry_news_dict)\n",
    "    await cloud_upload(db, total_news_dict, \"recent_news\")\n",
    "    \n",
    "async def upload_user_ticker_pref():\n",
    "    global db, user_ticker_pref_dict\n",
    "    await cloud_upload(db, user_ticker_pref_dict, \"user_ticker_pref\")\n",
    "    \n",
    "async def upload_preference_scores_user_rank():\n",
    "    global db, preference_scores_user_rank\n",
    "    await cloud_upload(db, preference_scores_user_rank, \"preference_scores_user_rank\")\n",
    "    \n",
    "def upload_time_tracker(full_update=True):\n",
    "    global db\n",
    "    if full_update:\n",
    "        cloud_upload_single(db, \"#update_time\", \"data_update\", {\"time\":datetime.datetime.now()})\n",
    "    else:\n",
    "        cloud_upload_single(db, \"#update_time\", \"user_pref_update\", {\"time\":datetime.datetime.now()})\n",
    "\n",
    "def upload_ticker_info(db):\n",
    "    global ticker_info_dict\n",
    "    cloud_upload_seq(db, \"ticker_info\", ticker_info_dict)\n",
    "    \n",
    "async def upload_new_user_pref():\n",
    "    global news_data_dict, industry_news_dict, user_ticker_pref_dict, industry_news_compiled_dict, news_datetime_dict, industry_news_hash_dict, preference_scores_user_rank\n",
    "    user_ticker_pref_dict = user_base_pref_gen()\n",
    "    industry_news_compiled_dict, news_datetime_dict = get_industry_news()\n",
    "    industry_news_hash_dict = industry_news_to_hash()\n",
    "    preference_scores_user_rank = get_user_news_rank()\n",
    "    #upload data\n",
    "    await upload_user_ticker_pref()\n",
    "    await upload_preference_scores_user_rank()\n",
    "    upload_time_tracker(full_update=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c95e17c7-3586-49a9-9bfe-729b9463478c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_dict(d):\n",
    "    flattened = {}\n",
    "    for key in d:\n",
    "        if isinstance(d[key], dict):\n",
    "            for sub_key in d[key]:\n",
    "                flattened[f\"{key}.{sub_key}\"] = d[key][sub_key]\n",
    "        else:\n",
    "            flattened[key] = d[key]\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edf2ac4e-aa31-4007-b0d7-a270aaf07dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading:   0%|                                                                                                          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tqdm.std.tqdm at 0x7fcc0f237f40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.44s/it]\n",
      "Uploading:   0%|                                                                                                          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tqdm.std.tqdm at 0x7fcc0f237f40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.28s/it]\n",
      "Uploading:   0%|                                                                                                          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tqdm.std.tqdm at 0x7fca9f27bca0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.48s/it]\n",
      "Uploading:   0%|                                                                                                          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tqdm.std.tqdm at 0x7fca9ee63a30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "await frequent_update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87afe02e-bcd4-4f0d-9a5c-e73241801dde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading:   0%|                                                                                                          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tqdm.std.tqdm at 0x7fcc22479d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.28s/it]\n",
      "Uploading:   0%|                                                                                                          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tqdm.std.tqdm at 0x7fcc22479d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "await upload_new_user_pref()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a919cac8-1ff5-497e-a20f-71c794e798bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#upload functions\n",
    "def cloud_upload_single(db, collection_name, doc_name, data_dict):\n",
    "    #upload data to db\n",
    "    doc_ref = db.collection(collection_name).document(doc_name)\n",
    "    doc_ref.set(data_dict)\n",
    "\n",
    "def chunks(data, size):\n",
    "    data_keys = list(data.keys())\n",
    "    for i in range(0, len(data_keys), size):\n",
    "        yield {k: data[k] for k in data_keys[i:i+size]}\n",
    "        \n",
    "def cloud_upload_seq(db, collection_name, data_dict, chunk_size=500):\n",
    "    sub_dicts = list(chunks(data_dict, chunk_size))\n",
    "    # Iterate through the sub_dicts\n",
    "    for sub_dict in sub_dicts:\n",
    "        # Create a batch to batch the writes\n",
    "        batch = db.batch()\n",
    "        # Iterate through the key-value pairs in the sub_dict\n",
    "        for key, value in sub_dict.items():\n",
    "            # Set the document reference in the news_data collection using the key\n",
    "            doc_ref = db.collection(collection_name).document(str(key))\n",
    "            # Add the key-value pair to the batch\n",
    "            batch.set(doc_ref, value)\n",
    "        # Commit the batch\n",
    "        batch.commit()\n",
    "        \n",
    "async def set_document(doc_ref, data):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        await asyncio.get_event_loop().run_in_executor(executor, doc_ref.set, data)\n",
    "\n",
    "async def write_document(doc_ref, data):\n",
    "    \"\"\"\n",
    "    a coroutine for writing a document to Firestore\n",
    "    \"\"\"\n",
    "    await set_document(doc_ref, data)\n",
    "\n",
    "async def write_batch(batch, collection_name, task_limit=12):\n",
    "    \"\"\"\n",
    "    write a batch of documents to Firestore asynchronously\n",
    "    \"\"\"\n",
    "    semaphore = asyncio.Semaphore(task_limit) # Create a semaphore to limit the number of concurrent tasks\n",
    "    coroutines = [] # Create a list to hold the coroutines\n",
    "    # Iterate through the batch and create a coroutine for each document\n",
    "    for key, value in batch.items():\n",
    "        # Set the document reference in the news_data collection using the key\n",
    "        doc_ref = db.collection(collection_name).document(str(key))\n",
    "        # Create a coroutine to write the document to Firestore\n",
    "        coroutine = write_document(doc_ref, value)\n",
    "        # Append the coroutine to the list\n",
    "        coroutines.append(coroutine)\n",
    "    # Run the coroutines concurrently with the semaphore\n",
    "    async with semaphore:\n",
    "        await asyncio.gather(*coroutines)\n",
    "\n",
    "async def cloud_upload(db, data_dict, collection_name, chunk_size=500, task_limit=12):\n",
    "    \"\"\"\n",
    "    upload data dict to collection\n",
    "    \"\"\"\n",
    "    sub_dicts = list(chunks(data_dict, chunk_size))\n",
    "    \n",
    "    # Create a progress bar for displaying the progress\n",
    "    progress_bar = tqdm(total=len(sub_dicts), desc=\"Uploading\")\n",
    "    display(progress_bar)\n",
    "\n",
    "    # Iterate through the sub_dicts\n",
    "    for sub_dict in sub_dicts:\n",
    "        batch = {}  # Create a batch to batch the writes\n",
    "        for key, value in sub_dict.items():  # Iterate through the key-value pairs in the sub_dict\n",
    "            batch[key] = value  # Add the key-value pair to the batch\n",
    "        await write_batch(batch, collection_name, task_limit=task_limit)  # Run the write_batch coroutine asynchronously\n",
    "        progress_bar.update(1)  # Update the progress bar\n",
    "\n",
    "    progress_bar.close()  # Close the progress bar when done\n",
    "        \n",
    "        \n",
    "def delete_collection(db, collection_path, batch_size=500):\n",
    "    \"\"\"\n",
    "    Delete all documents in a collection in batches to avoid exceeding the maximum write rate or request size limit.\n",
    "    \"\"\"\n",
    "    query = db.collection(collection_path).limit(batch_size) # Create a query for the collection\n",
    "    has_docs = True # Set a flag to determine if the loop should continue\n",
    "    while has_docs: # Delete documents in batches until no more documents exist\n",
    "        docs = query.stream() # Get a batch of documents\n",
    "        has_docs = False # Set has_docs to False, assuming there are no more documents\n",
    "        batch = db.batch() # Create a batch to delete documents\n",
    "        for doc in docs: # Iterate through documents and delete them in the batch\n",
    "            batch.delete(doc.reference)\n",
    "            has_docs = True # If a document is found, set has_docs to True\n",
    "        batch.commit() # Commit the batch\n",
    "        if not has_docs: # If no documents were found, break out of the loop\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de023d6c-e68f-4ad0-87eb-414520854d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sic matching and ticker map cloud set\n",
    "\n",
    "def cloud_upload_ticker_map(db):\n",
    "    \"\"\"\n",
    "    upload and set ticker map (str->int) to cloud firestore \n",
    "    \"\"\"\n",
    "    ticker_map_path = \"/mnt/d/data/news/ticker_maping_dict.pkl\"\n",
    "    ticker_map_dict = pickle.load(open(ticker_map_path, \"rb\"))\n",
    "    #overwrite ticker mapping on db\n",
    "    doc_ref = db.collection('ticker_map').document('dict')\n",
    "    doc_ref.set(ticker_map_dict)\n",
    "\n",
    "def memoize(function):\n",
    "    \"\"\"\n",
    "    cache helper for speed optimization\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    def wrapper(input):\n",
    "        if input not in cache:\n",
    "            cache[input] = function(input)\n",
    "        return cache[input]\n",
    "    return wrapper\n",
    "\n",
    "@memoize\n",
    "def sic_match(input):\n",
    "    \"\"\"\n",
    "    takes a SIC code and return the 10 SIC industries string\n",
    "    \"\"\"\n",
    "    sic_codes = {\n",
    "        '01': 'agriculture',\n",
    "        '02': 'agriculture',\n",
    "        '07': 'agriculture',\n",
    "        '08': 'agriculture',\n",
    "        '09': 'agriculture',\n",
    "        '10': 'mining',\n",
    "        '11': 'mining',\n",
    "        '12': 'mining',\n",
    "        '13': 'mining',\n",
    "        '14': 'mining',\n",
    "        '15': 'construction',\n",
    "        '16': 'construction',\n",
    "        '17': 'construction',\n",
    "        **{f\"{i:02d}\": \"manufacturing\" for i in range(20, 40)},\n",
    "        **{f\"{i:02d}\": \"transportation\" for i in range(40, 50)},\n",
    "        '50': 'wholesale',\n",
    "        '51': 'wholesale',\n",
    "        **{f\"{i:02d}\": \"retail\" for i in range(52, 60)},\n",
    "        **{f\"{i:02d}\": \"finance\" for i in range(60, 68)},\n",
    "        **{f\"{i:02d}\": \"services\" for i in range(70, 90)},\n",
    "        **{f\"{i:02d}\": \"public_administration\" for i in range(91, 100)},\n",
    "    }\n",
    "    try:\n",
    "        return sic_codes[str(input)[0:2]]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid input. Please enter a two-character string matching a valid SIC code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deb7f1db-05cd-497e-9a31-ea0cd63a56d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate ticker info dict\n",
    "\n",
    "async def get_all_ticker_info(db):\n",
    "    ticker_map_dict = db.collection('tickers').document('ticker_hash').get().to_dict()\n",
    "    url_factory = poly_url.StockUrlFactory(api_key)\n",
    "    ticker_lc = ticker_map_dict.keys()\n",
    "    urls_dict = {ticker: url_factory.ReferenceData.ticker_info(url_factory, ticker) for ticker in ticker_lc}\n",
    "    df_dict = await poly_helper.get_data_from_urls(urls_dict)\n",
    "    return df_dict\n",
    "\n",
    "async def ticker_info_ready(db):\n",
    "    df_info_dict = await get_all_ticker_info(db)\n",
    "    upsert_dict = {ticker: df.to_dict('records')[0] for ticker, df in df_info_dict.items()}\n",
    "    return upsert_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5608f520-781a-4539-b8d8-f9b03f9ecd91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def info_dict_to_sic_map(ticker_info_dict, ticker_hash_dict):\n",
    "    sic_map_dict = {\n",
    "        ticker_hash_dict[str(ticker)]: sic_match(info[\"sic_code\"][:2]) if (\"sic_code\" in info.keys()) else None\n",
    "        for ticker, info in ticker_info_dict.items()\n",
    "    }\n",
    "    return sic_map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc062ac5-cb3b-4d09-9216-63d7f60da965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#generate sic map\n",
    "#ticker_info_dict = await ticker_info_ready(db)\n",
    "#ticker_hash_dict = db.collection('tickers').document('ticker_hash').get().to_dict()\n",
    "#sic_map_dict = info_dict_to_sic_map(ticker_info_dict, ticker_hash_dict)\n",
    "#cloud_upload_single(db, \"tickers\", \"hash_sic\", sic_map_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21cda6f-3c62-4510-b8f9-76bb252d0211",
   "metadata": {
    "tags": []
   },
   "source": [
    "upload news data to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27bfe880-a0a5-4b6c-84f1-0ee9ac8e56cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload news data to db\n",
    "\n",
    "def convert_arrays_to_lists(value):\n",
    "    \"\"\"\n",
    "    Convert arrays to lists\n",
    "    \"\"\"\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return list(value)\n",
    "    return value\n",
    "\n",
    "def to_boolean_list(industries):\n",
    "    \"\"\"\n",
    "    convert the industries list to a boolean list\n",
    "    \"\"\"\n",
    "    global industry_cols\n",
    "    return [col in industries for col in industry_cols]\n",
    "\n",
    "def clean_news(news_data=pd.read_feather(\"/mnt/d/data/news/local_us_equity_news\")):\n",
    "    \"\"\"depreciated\"\"\"\n",
    "    cols_to_process = ['tickers', 'keywords']\n",
    "    for col in cols_to_process:\n",
    "        news_data[col] = news_data[col].apply(lambda lst:  tuple(lst) if isinstance(lst, list) else None)\n",
    "    if \"publisher\" in news_data.columns:\n",
    "        flattened_info=json_normalize(news_data[\"publisher\"])\n",
    "        flattened_info.reset_index(drop=True, inplace=True)\n",
    "        news_data.reset_index(drop=True, inplace=True)\n",
    "        news_data=pd.concat([news_data.drop('publisher', axis=1), flattened_info], axis=1)\n",
    "    news_data.drop_duplicates(inplace=True)\n",
    "    return news_data\n",
    "\n",
    "def apply_industries(news_data):\n",
    "    global hash_sic_dict,ticker_hash_dict\n",
    "    #add 10 industry cols\n",
    "    news_data[\"industries\"] = news_data.tickers.apply(lambda tickers: [hash_sic_dict.get(ticker_hash_dict.get(ticker,None), None) for ticker in tickers])\n",
    "    news_data[\"industries\"] = news_data[\"industries\"].apply(lambda lst:  tuple(lst) if lst is not None else None)\n",
    "    # Define the industry column names and default values\n",
    "    industry_cols = list(set(['agriculture', 'mining', 'construction', 'manufacturing', 'transportation',\n",
    "                     'wholesale', 'retail', 'finance', 'services', 'public_administration']))\n",
    "    # Create a dataframe with the boolean values for each industry\n",
    "    boolean_df = pd.DataFrame(tqdm(news_data['industries'].apply(to_boolean_list).tolist()), columns=industry_cols)\n",
    "    # process the news_data df\n",
    "    news_data = news_data.reset_index(drop=True)\n",
    "    boolean_df = boolean_df.reset_index(drop=True)\n",
    "    news_data = pd.concat([news_data, boolean_df], axis=1)\n",
    "    news_data = news_data.applymap(convert_arrays_to_lists)\n",
    "    return news_data\n",
    "\n",
    "def process_news_data():\n",
    "    global db\n",
    "    news_data = clean_news()\n",
    "    #get mapping dict\n",
    "    doc_ref = db.collection('tickers').document('hash_sic') # Get reference to the document\n",
    "    hash_sic_doc = doc_ref.get() # Retrieve the document data\n",
    "    hash_sic_dict = hash_sic_doc.to_dict() if hash_sic_doc.exists else print(f\"No such document: {doc_ref.id}\") # Check if the document exists\n",
    "    doc_ref = db.collection('tickers').document('ticker_hash') # Get reference to the document\n",
    "    ticker_hash_doc = doc_ref.get() # Retrieve the document data\n",
    "    ticker_hash_dict = ticker_hash_doc.to_dict() if ticker_hash_doc.exists else print(f\"No such document: {doc_ref.id}\") # Check if the document exists\n",
    "    return apply_industries(news_data, hash_sic_dict,ticker_hash_dict)\n",
    "\n",
    "def news_data_to_dict(news_data):\n",
    "    # Convert the DataFrame to a dictionary format\n",
    "    news_data_dict = news_data.set_index('id').T.to_dict()\n",
    "    return news_data_dict\n",
    "\n",
    "def gen_10_industries_df():\n",
    "    global news_data_dict, industry_cols\n",
    "    industry_data = {} # Create a dictionary to hold the smaller dataframes for each industry\n",
    "    for industry_col in industry_cols: # Iterate through the industry columns\n",
    "        industry_news = {} # Create a dictionary to hold the most recent 100 news items for this industry\n",
    "        for news_hash_id, news_data in news_data_dict.items(): # Iterate through all news items\n",
    "            industries = news_data.get('industries', [])\n",
    "            if industry_col in industries: # Check if the news item belongs to the current industry\n",
    "                industry_news[news_hash_id] = news_data # Add the news item to the industry_news dictionary\n",
    "        industry_news = dict(sorted(industry_news.items(), key=lambda x: x[1]['published_utc'], reverse=True)[:100]) # Sort the dictionary by published_utc and take the most recent 100 items\n",
    "        industry_data[industry_col] = industry_news # Add the industry_news dictionary to the industry_data dictionary\n",
    "        #print(f\"{industry_col}: {len(industry_news)} rows added\")\n",
    "    return industry_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c6729e2-ce65-4673-aa64-1da614b4afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map ticker to industry (OK)\n",
    "#get tickers col as list from master news dataframe (rows) (OK)\n",
    "#map tickers in list 10 industries boolean col (OK)\n",
    "#create 10 industry dataframes, 100 rows each, order by time (OK)\n",
    "#write a python program that run every x mins to get new news and append to the master dataframe, and also append to the 10 industries dataframes.\n",
    "#user requests for news -> look up pref in firebase by user hashid -> merge industries dataframes -> push to front end 100 rows but limit displace 10 rows at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dba62c6c-41c8-4efc-b1d3-289f3c795273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def target_ticker_pair_gen(news_data_dict, ticker_hash_dict):\n",
    "    target_data_dict = {\n",
    "        key: tuple(\n",
    "            itertools.combinations(\n",
    "                tuple(\n",
    "                    ticker_hash_dict.get(ticker, None)\n",
    "                    for ticker in value['tickers']\n",
    "                    if ticker_hash_dict.get(ticker, None) is not None\n",
    "                ),\n",
    "                2\n",
    "            )\n",
    "        )\n",
    "        for key, value in news_data_dict.items()\n",
    "    }\n",
    "    # Flatten target_data\n",
    "    target_data = tuple(itertools.chain.from_iterable(target_data_dict.values()))\n",
    "    return target_data\n",
    "\n",
    "def knn_gen(target_data, hash_sic_dict, ticker_hash_dict):\n",
    "    knn_dict = {key: {inner_key: 0 for inner_key in hash_sic_dict} for key in hash_sic_dict}\n",
    "    for pair in tqdm(target_data):\n",
    "        first_item, second_item = pair\n",
    "        knn_dict[first_item][second_item] += 1\n",
    "        knn_dict[second_item][first_item] += 1\n",
    "    return knn_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f533b56-a2e1-4256-bc34-949b61fd12c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ticker_map_dict_gen():\n",
    "    # Define a function to create the hashid\n",
    "    def create_hashid(row):\n",
    "        cik = row['cik'] if row['cik'] else ''\n",
    "        composite_figi = row['composite_figi'] if row['composite_figi'] else ''\n",
    "        hash_str = f\"{row['ticker']}{cik}{composite_figi}\"\n",
    "        return hashlib.md5(hash_str.encode()).hexdigest()\n",
    "\n",
    "    #get tickers on us equity market:\n",
    "    url_factory = poly_url.StockUrlFactory(api_key)\n",
    "    url = url_factory.ReferenceData.tickers(url_factory)\n",
    "    tickers =  poly_helper.get_data_from_single_url(url)\n",
    "    # Apply the function to each row to create a hashid column\n",
    "    tickers['hashid'] = tickers.apply(create_hashid, axis=1)\n",
    "    ticker_map_dict = tickers[['ticker', 'hashid']].set_index('ticker').to_dict()['hashid']\n",
    "    return ticker_map_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6e2bcc8-050c-4a9f-afad-1176a7326248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ticker_map_dict = tickers[['ticker', 'hashid']].set_index('ticker').to_dict()['hashid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14ea2f64-05fc-4a78-a9f3-2fcbdb540cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cloud_upload_single(db, \"tickers\", \"ticker_hash\", ticker_map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30e0429f-eaf4-44af-aac4-5fc149bc4a5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cloud_upload_single(db, \"#update_time\", \"time_tracker\", {\"backend_code\":datetime.datetime.now()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeddcba0-f55f-4138-b951-303ebeb32698",
   "metadata": {},
   "source": [
    "user data feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9396bc9a-7af5-4b70-9fa0-b6e18b575fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def user_base_pref_gen():\n",
    "    global db, hash_sic_dict\n",
    "    user_pref_docs = db.collection('user_preferences').stream()  # Get all documents from the user_preference collection\n",
    "    user_ticker_pref_dict = {}  # Initialize an empty dictionary to store user preferences for each ticker\n",
    "    for doc in user_pref_docs:  # Iterate through each user preference document\n",
    "        user_id = doc.id\n",
    "        user_pref = doc.to_dict()  # Get the user preference dictionary from the document data\n",
    "        pref_dict = {key: 0 for key in hash_sic_dict} # Initialize a new pref_dict with the same keys as hash_sic_dict and default values of 0\n",
    "        # loop\n",
    "        for pref_industry, pref_value in user_pref.items():\n",
    "            for ticker_hash_id, sic_code in hash_sic_dict.items():\n",
    "                if sic_code == pref_industry:\n",
    "                    if pref_value==True:\n",
    "                        pref_dict[ticker_hash_id] += 1\n",
    "                    else:\n",
    "                        pref_dict[ticker_hash_id] -= 1\n",
    "        # Store the pref_dict in user_ticker_pref\n",
    "        user_ticker_pref_dict[user_id] = pref_dict\n",
    "    return user_ticker_pref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f80f7bb4-1fb0-42b7-b2cf-f614de00b04a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#user_ticker_pref_dict = user_base_pref_gen()\n",
    "#await cloud_upload(db, user_ticker_pref_dict, \"user_ticker_pref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbfa4c9c-4792-4ed0-b1ac-9656a373e8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_industry_news():\n",
    "    global db, industry_news_dict\n",
    "    news_datetime_dict = {news_hash_id: datetime.datetime.fromisoformat(news_data['published_utc'][:-1]).replace(tzinfo=pytz.UTC)\n",
    "                      for industry_data in industry_news_dict.values() for news_hash_id, news_data in industry_data.items()}\n",
    "    result_dict = {}\n",
    "    for sub_dict in industry_news_dict.values():\n",
    "        result_dict.update(sub_dict)\n",
    "    return result_dict, news_datetime_dict\n",
    "\n",
    "def industry_news_to_hash():\n",
    "    global industry_news_compiled_dict, ticker_hash_dict\n",
    "    return {\n",
    "        key: {ticker_hash_dict.get(ticker) for ticker in value[\"tickers\"] if ticker_hash_dict.get(ticker)}\n",
    "        for key, value in industry_news_compiled_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d984b63c-105b-49b1-9543-098a2aa307d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#industry_news_compiled_dict, news_datetime_dict = get_industry_news()\n",
    "#industry_news_hash_dict = industry_news_to_hash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7074fbd-047a-4170-aa6c-fa4fe92aaac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pref_score_gen():\n",
    "    global industry_news_hash_dict,user_ticker_pref_dict\n",
    "    preference_scores = {} # Initialize a dictionary to store preference scores for each news article and user\n",
    "    for news_hash_id, ticker_hash_ids in industry_news_hash_dict.items(): # Iterate through each news article and its associated tickers\n",
    "        for user_hash_id, user_pref_dict in user_ticker_pref_dict.items(): # Iterate through each user and their associated preference dictionary\n",
    "            user_article_scores = [] # Initialize a list to store preference scores for this user and article\n",
    "            for ticker_hash_id in ticker_hash_ids: # Iterate through each ticker in the article's ticker list\n",
    "                pref_score = user_pref_dict.get(ticker_hash_id, 0) # Lookup the preference score for this ticker for the current user\n",
    "                user_article_scores.append(pref_score) # Add the preference score to the list for this article\n",
    "            avg_score = sum(user_article_scores) / len(user_article_scores) # Calculate the average preference score for this user and article\n",
    "            if news_hash_id not in preference_scores: # Store the preference score in the dictionary\n",
    "                preference_scores[news_hash_id] = {}\n",
    "            preference_scores[news_hash_id][user_hash_id] = avg_score\n",
    "    return preference_scores\n",
    "        \n",
    "def flip_dict(dict1):\n",
    "    dict2 = {}\n",
    "    for parent_key, sub_dict in dict1.items():\n",
    "        for sub_key, sub_value in sub_dict.items():\n",
    "            if sub_key not in dict2:\n",
    "                dict2[sub_key] = {}\n",
    "            dict2[sub_key][parent_key] = sub_value\n",
    "    return dict2\n",
    "\n",
    "def scale_pref_scores(preference_scores):\n",
    "    global news_datetime_dict\n",
    "    now_utc = datetime.datetime.utcnow().replace(tzinfo=pytz.UTC)\n",
    "    for news_hash_id, news_datetime in news_datetime_dict.items(): # Loop over all news articles\n",
    "        delta_time = (now_utc - news_datetime).total_seconds() # Compute the time difference between the article and the current time in seconds\n",
    "        scale_factor = min(1, 4/np.log1p(delta_time)) # Compute the scaling factor\n",
    "        for user_hash_id in preference_scores[news_hash_id]: # Loop over all users for this news article\n",
    "            if preference_scores[news_hash_id][user_hash_id] !=0:\n",
    "                preference_scores[news_hash_id][user_hash_id] *= scale_factor # Scale the preference score for this ticker\n",
    "    return preference_scores\n",
    "\n",
    "def sort_subdict_by_value(d):\n",
    "    return {k: dict(sorted(v.items(), key=lambda x: x[1], reverse=True)) for k, v in d.items()}\n",
    "\n",
    "def get_user_news_rank():\n",
    "    preference_scores = pref_score_gen()\n",
    "    preference_scores = scale_pref_scores(preference_scores)\n",
    "    preference_scores_user_rank = flip_dict(preference_scores)\n",
    "    preference_scores_user_rank = sort_subdict_by_value(preference_scores_user_rank)\n",
    "    return preference_scores_user_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "540c48d3-2ed1-4d13-be8e-bfbaa404ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_keys(user_preferences):\n",
    "    corrected_keys = {\n",
    "        'algriculture': 'agriculture',\n",
    "        'construction': 'construction',\n",
    "        'transportation': 'transportation',\n",
    "        'manufacuring': 'manufacturing',\n",
    "        'wholesale': 'wholesale',\n",
    "        'public_administration': 'public_administration',\n",
    "        'mining': 'mining',\n",
    "        'finance': 'finance',\n",
    "        'retail': 'retail',\n",
    "        'services': 'services'\n",
    "    }\n",
    "\n",
    "    return {corrected_keys[key]: value for key, value in user_preferences.items()}\n",
    "\n",
    "def correct_user_pref_key():\n",
    "    \"\"\"\n",
    "    no need to call it in the future, one time fix of user pref collection's dict keys\n",
    "    \"\"\"\n",
    "    user_preferences_ref = db.collection('user_preferences')\n",
    "    for doc in user_preferences_ref.stream():\n",
    "        user_preferences = doc.to_dict()\n",
    "        corrected_preferences = rename_keys(user_preferences)\n",
    "        # Update the document with the corrected key-value pairs\n",
    "        doc_ref = user_preferences_ref.document(doc.id)\n",
    "        doc_ref.set(corrected_preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4710c1ef-9e27-48ba-bfde-f9f41dd2607c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_industries(news_data_dict, hash_sic_dict, ticker_hash_dict):\n",
    "    industry_cols = list(set(['agriculture', 'mining', 'construction', 'manufacturing', 'transportation',\n",
    "                              'wholesale', 'retail', 'finance', 'services', 'public_administration']))\n",
    "    for news_hash_id, news_data in news_data_dict.items():\n",
    "        tickers = news_data.get('tickers', [])\n",
    "        industries = [hash_sic_dict.get(ticker_hash_dict.get(ticker, None), None) for ticker in tickers]\n",
    "        news_data['industries'] = tuple(industries) if industries else None\n",
    "        boolean_values = to_boolean_list(news_data['industries'])\n",
    "        news_data.update({col: val for col, val in zip(industry_cols, boolean_values)})\n",
    "    return news_data_dict\n",
    "\n",
    "async def get_news_dict(ticker_hash_dict):\n",
    "    url_factory = poly_url.StockUrlFactory(api_key)\n",
    "    ticker_lc = ticker_hash_dict.keys()\n",
    "    doc_ref = db.collection(\"#update_time\").document(\"data_update\") # Get reference to the document\n",
    "    t0_update = doc_ref.get() # Retrieve the document data\n",
    "    update_date = t0_update.get(\"time\")  # assuming the date field in the document is called \"date\"\n",
    "    date_str = update_date.strftime(\"%Y-%m-%d\")\n",
    "    urls_dict = {ticker: url_factory.ReferenceData.news(url_factory, ticker, publish_utc_gte=date_str) for ticker in ticker_lc}\n",
    "    news_dict = await poly_helper.get_data_from_urls(urls_dict)\n",
    "    recent_news_df = pd.concat(news_dict)\n",
    "    recent_news_df.reset_index(inplace=True,drop=True)\n",
    "    recent_news_df = clean_news(recent_news_df)\n",
    "    recent_news_df.set_index('id', inplace=True)\n",
    "    return recent_news_df.T.to_dict()\n",
    "    \n",
    "def update_news_data_dict(news_data_dict, recent_news_dict):\n",
    "    news_data_dict.update(recent_news_dict)\n",
    "    return dict(sorted(news_data_dict.items(), key=lambda x: x[1][\"published_utc\"], reverse=True))\n",
    "\n",
    "async def run_news_update():\n",
    "    global news_data_dict, ticker_hash_dict, hash_sic_dict\n",
    "    recent_news_dict = await get_news_dict(ticker_hash_dict)\n",
    "    recent_news_dict = apply_industries(recent_news_dict, hash_sic_dict, ticker_hash_dict)\n",
    "    news_data_dict = update_news_data_dict(news_data_dict, recent_news_dict)\n",
    "    return news_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "150a2a80-ad88-470d-b40b-502dd57c9394",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_n_item_in_dict(mydict,n=1):\n",
    "    if n>0:\n",
    "        return {k: v for idx, (k, v) in enumerate(mydict.items()) if idx < 1}\n",
    "    elif n<0:\n",
    "        return {k: news_data_dict[k] for k in list(news_data_dict)[-1:]}\n",
    "    else:\n",
    "        print(\"n can't be 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7901790f-6c7f-4e54-8c6a-7e6b1dd9d601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#show_n_item_in_dict(industry_news_hash_dict,n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1432644f-c574-49dd-8387-1c0dd7484022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_by_value(d, value):\n",
    "    for k, v in d.items():\n",
    "        if v == value:\n",
    "            return k\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea9929d-c293-4ee6-82c2-0c4fba5fb403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_kernel",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
